# λ…Όλ¬Έ μ¬ν„μ„± κ²€μ¦ λ³΄κ³ μ„

## λ©ν‘
λ…Όλ¬Έ "Temporal Difference Models: Model-Free Deep RL for Model-Based Control" (ICLR 2018, arXiv:1802.09081)μ μ‹¤ν— ν™κ²½κ³Ό λ΅μ§μ„ λ¨λ‘ μ‚¬μ©ν•  μ μλ”μ§€ κ²€μ¦

## κ²€μ¦ ν•­λ©

### 1. ν•µμ‹¬ μ•κ³ λ¦¬μ¦ κµ¬ν„ β“

#### 1.1 TDM Q-function
- **λ…Όλ¬Έ**: `Q(s, a, sg, Ο„) = -||f(s, a, sg, Ο„) - sg||`
- **κµ¬ν„ μƒνƒ**: β… `tdm.py`μ `TDMCritic.compute_q_value()`μ— μ •ν™•ν κµ¬ν„λ¨
- **κ²€μ¦**: μ½”λ“ ν™•μΈ μ™„λ£

#### 1.2 Goal-conditioned Policy (Actor)
- **λ…Όλ¬Έ**: `Ο€(a|s, g, Ο„)` ν•νƒμ goal-conditioned policy
- **κµ¬ν„ μƒνƒ**: β… `networks.py`μ `Actor` ν΄λμ¤κ°€ state, goal, tauλ¥Ό λ¨λ‘ μ…λ ¥μΌλ΅ λ°›μ
- **κ²€μ¦**: `Actor.forward(state, goal, tau)` κµ¬ν„ ν™•μΈ μ™„λ£

#### 1.3 TDM Loss (Bellman Equation)
- **λ…Όλ¬Έ Equation (5)**: 
  ```
  Q(s, a, sg, Ο„) = E[-D(s', sg)Β·1[Ο„=0] + max_a' Q(s', a', sg, Ο„-1)Β·1[Ο„β‰ 0]]
  ```
- **κµ¬ν„ μƒνƒ**: β… `tdm.py`μ `update_critic()`μ— κµ¬ν„λ¨
- **κ²€μ¦**: tau_maskλ¥Ό μ‚¬μ©ν• μ΅°κ±΄λ¶€ κ³„μ‚° ν™•μΈ μ™„λ£

#### 1.4 Vectorized Supervision
- **λ…Όλ¬Έ Appendix A.5**: κ° μ°¨μ›μ„ λ…λ¦½μ μΌλ΅ supervision
- **κµ¬ν„ μƒνƒ**: β… `TDMCriticVectorized` ν΄λμ¤μ™€ vectorized loss κ³„μ‚° κµ¬ν„λ¨
- **κ²€μ¦**: `update_critic()`μ—μ„ vectorized λ¨λ“ ν™•μΈ μ™„λ£

### 2. Goal Relabeling β“

#### 2.1 Future State Sampling
- **λ…Όλ¬Έ**: κ° transitionμ„ μ—¬λ¬ λ©ν‘μ™€ μ§€ν‰μ„ μΌλ΅ relabel
- **κµ¬ν„ μƒνƒ**: β… `replay_buffer.py`μ `GoalRelabeler` ν΄λμ¤μ— κµ¬ν„λ¨
- **μ „λµ**: 'future', 'buffer', 'uniform' μ§€μ›
- **κ²€μ¦**: `sample_tdm_batch()`μ—μ„ goal relabeling ν™•μΈ μ™„λ£

#### 2.2 Horizon Relabeling
- **λ…Όλ¬Έ**: Ο„λ¥Ό 0λ¶€ν„° Ο„_maxκΉμ§€ μƒν”λ§
- **κµ¬ν„ μƒνƒ**: β… `GoalRelabeler.relabel()`μ—μ„ Ο„λ¥Ό λλ¤ μƒν”λ§
- **κ²€μ¦**: μ½”λ“ ν™•μΈ μ™„λ£

### 3. MPC κΈ°λ° μ •μ±… μ¶”μ¶ β“

#### 3.1 Direct Policy Extraction
- **λ…Όλ¬Έ Equation (9)**: `a* = argmax_a Q(s, a, g, tau)`
- **κµ¬ν„ μƒνƒ**: β… `mpc_planner.py`μ `plan_direct()` λ©”μ„λ“
- **κ²€μ¦**: Actor λ„¤νΈμ›ν¬λ¥Ό μ§μ ‘ μ‚¬μ©ν•μ—¬ κµ¬ν„ ν™•μΈ

#### 3.2 Optimization-based Extraction
- **λ…Όλ¬Έ Equation (8)**: ν™•λ¥ μ  μµμ ν™”λ¥Ό ν†µν• action μ„ νƒ
- **κµ¬ν„ μƒνƒ**: β… `plan_optimization()` λ©”μ„λ“
- **κ²€μ¦**: μƒν”λ§ κΈ°λ° μµμ ν™” κµ¬ν„ ν™•μΈ

#### 3.3 Task-specific Planning
- **λ…Όλ¬Έ Appendix**: ν™κ²½λ³„ μµμ ν™” λ°©λ²•
- **κµ¬ν„ μƒνƒ**: β… `TaskSpecificPlanner` ν΄λμ¤
- **κ²€μ¦**: Reacher, Pusher, HalfCheetah, Ant μ§€μ› ν™•μΈ

### 4. μ‹¤ν— ν™κ²½ β“

#### 4.1 μ§€μ› ν™κ²½
λ…Όλ¬Έμ—μ„ μ‚¬μ©ν• ν™κ²½λ“¤μ΄ λ¨λ‘ κµ¬ν„λμ–΄ μμ:

| ν™κ²½ | λ…Όλ¬Έ | κµ¬ν„ μƒνƒ | Goal μ¶”μ¶ |
|------|------|----------|-----------|
| Reacher | β“ | β… Reacher-v5 | End-effector μ„μΉ (2D) |
| Pusher | β“ | β… Pusher-v5 | Hand + Puck XY |
| HalfCheetah | β“ | β… HalfCheetah-v5 | μ†λ„ |
| Ant | β“ | β… Ant-v5 | μ„μΉ λλ” μ„μΉ+μ†λ„ |

#### 4.2 Goal Space
- **λ…Όλ¬Έ**: κ° ν™κ²½μ— λ§λ” goal space μ •μ
- **κµ¬ν„ μƒνƒ**: β… `env_wrapper.py`μ `GoalExtractor` ν΄λμ¤
- **κ²€μ¦**: λ¨λ“  ν™κ²½μ— λ€ν• goal μ¶”μ¶ λ΅μ§ ν™•μΈ μ™„λ£

### 5. ν•™μµ μ•κ³ λ¦¬μ¦ β“

#### 5.1 Replay Buffer
- **λ…Όλ¬Έ**: Off-policy ν•™μµμ„ μ„ν• experience replay
- **κµ¬ν„ μƒνƒ**: β… `TDMBuffer` ν΄λμ¤ (ν¬κΈ°: 1M)
- **κ²€μ¦**: Goal relabelingκ³Ό ν•¨κ» κµ¬ν„ ν™•μΈ

#### 5.2 Target Network
- **λ…Όλ¬Έ**: Soft target update (Polyak averaging)
- **κµ¬ν„ μƒνƒ**: β… `tdm.py`μ `update_target_networks()` λ©”μ„λ“
- **κ²€μ¦**: Polyak coefficient μ‚¬μ© ν™•μΈ

#### 5.3 Exploration
- **λ…Όλ¬Έ**: Action noiseλ¥Ό ν†µν• exploration
- **κµ¬ν„ μƒνƒ**: β… Gaussian noise μ¶”κ°€
- **κ²€μ¦**: `train.py`μ—μ„ noise μ¶”κ°€ ν™•μΈ

### 6. ν•μ΄νΌνλΌλ―Έν„° μ„¤μ • β οΈ

#### 6.1 λ…Όλ¬Έμ ν•μ΄νΌνλΌλ―Έν„°
λ…Όλ¬Έμ—λ” κµ¬μ²΄μ μΈ ν•μ΄νΌνλΌλ―Έν„° κ°’μ΄ λ…μ‹λμ–΄ μμ§€ μ•μ. μΌλ°μ μΈ λ²”μ„λ¥Ό μ‚¬μ©:

| ν•μ΄νΌνλΌλ―Έν„° | λ…Όλ¬Έ | ν„μ¬ κµ¬ν„ | μƒνƒ |
|---------------|------|----------|------|
| Learning rate (actor) | - | 0.0001 | β οΈ μΌλ°κ°’ μ‚¬μ© |
| Learning rate (critic) | - | 0.001 | β οΈ μΌλ°κ°’ μ‚¬μ© |
| tau_max | - | 25 | β οΈ μΌλ°κ°’ μ‚¬μ© |
| Batch size | - | 128 | β οΈ μΌλ°κ°’ μ‚¬μ© |
| Network size | 300x300 | 300x300 | β… μΌμΉ |
| Polyak | - | 0.999 | β οΈ μΌλ°κ°’ μ‚¬μ© |

#### 6.2 Grid Search μ§€μ›
- **κµ¬ν„ μƒνƒ**: β… `grid_search.py`λ΅ ν•μ΄νΌνλΌλ―Έν„° νƒμƒ‰ κ°€λ¥
- **κ²€μ¦**: λ…Όλ¬Έμ μ •ν™•ν• κ°’μ€ μ—†μ§€λ§, νƒμƒ‰μ„ ν†µν•΄ μ°Ύμ„ μ μμ

### 7. μ¶”κ°€ κΈ°λ¥ (μ¬ν„μ„± ν–¥μƒ) β“

#### 7.1 Curriculum Learning
- **λ…Όλ¬Έ**: λ…μ‹λμ§€ μ•μ
- **κµ¬ν„ μƒνƒ**: β… μ¶”κ°€ κΈ°λ¥μΌλ΅ κµ¬ν„
- **λ©μ **: μ¬ν„μ„± μ‹¤ν— μ‹ ν•™μµ μ•μ •μ„± ν–¥μƒ

#### 7.2 Warm-up Period
- **λ…Όλ¬Έ**: λ…μ‹λμ§€ μ•μ
- **κµ¬ν„ μƒνƒ**: β… μ¶”κ°€ κΈ°λ¥μΌλ΅ κµ¬ν„
- **λ©μ **: μ΄κΈ° ν•™μµ μ•μ •ν™”

#### 7.3 Early Stopping & Checkpointing
- **λ…Όλ¬Έ**: λ…μ‹λμ§€ μ•μ
- **κµ¬ν„ μƒνƒ**: β… κµ¬ν„λ¨
- **λ©μ **: μµκ³  μ„±λ¥ λ¨λΈ λ³΄μ΅΄

## μ¬ν„μ„± μ‹¤ν— μ¤€λΉ„ μƒνƒ

### β… μ™„μ „ν μ¤€λΉ„λ ν•­λ©

1. **ν•µμ‹¬ μ•κ³ λ¦¬μ¦**: TDM Q-function, Goal-conditioned policy, Loss κ³„μ‚°
2. **Goal Relabeling**: Future state sampling, Horizon relabeling
3. **MPC Planner**: Direct λ° Optimization κΈ°λ° μ •μ±… μ¶”μ¶
4. **μ‹¤ν— ν™κ²½**: Reacher, Pusher, HalfCheetah, Ant λ¨λ‘ μ§€μ›
5. **ν•™μµ νμ΄ν”„λΌμΈ**: Replay buffer, Target network, Exploration

### β οΈ μ£Όμκ°€ ν•„μ”ν• ν•­λ©

1. **ν•μ΄νΌνλΌλ―Έν„°**: λ…Όλ¬Έμ— λ…μ‹λ κ°’μ΄ μ—†μ–΄ μΌλ°μ μΈ λ²”μ„ μ‚¬μ©
   - **ν•΄κ²°μ±…**: Grid Searchλ¥Ό ν†µν•΄ μµμ κ°’ νƒμƒ‰ κ°€λ¥

2. **ν™κ²½ λ²„μ „**: λ…Όλ¬Έμ€ MuJoCo κΈ°λ° ν™κ²½ μ‚¬μ©, ν„μ¬λ” Gymnasium v5 μ‚¬μ©
   - **μν–¥**: API μ°¨μ΄λ΅ μΈν• λ―Έμ„Έν• μ°¨μ΄ κ°€λ¥
   - **ν•΄κ²°μ±…**: ν™κ²½ λνΌλ΅ λ€λ¶€λ¶„ ν•΄κ²°λ¨

### π“‹ μ¬ν„μ„± μ‹¤ν— μ²΄ν¬λ¦¬μ¤νΈ

#### ν•„μ μ‚¬ν•­
- [x] TDM μ•κ³ λ¦¬μ¦ κµ¬ν„
- [x] Goal-conditioned policy
- [x] Goal relabeling
- [x] MPC planner
- [x] μ‹¤ν— ν™κ²½ (4κ° λ¨λ‘)
- [x] ν•™μµ νμ΄ν”„λΌμΈ

#### κ¶μ¥ μ‚¬ν•­
- [x] Grid Search (ν•μ΄νΌνλΌλ―Έν„° νƒμƒ‰)
- [x] Early Stopping
- [x] Checkpointing
- [x] Curriculum Learning (μ„ νƒ)
- [x] Warm-up Period (μ„ νƒ)

## κ²°λ΅ 

### μ¬ν„μ„± μ‹¤ν— κ°€λ¥ μ—¬λ¶€: β… **κ°€λ¥**

**μ΄μ :**
1. λ…Όλ¬Έμ ν•µμ‹¬ μ•κ³ λ¦¬μ¦μ΄ λ¨λ‘ κµ¬ν„λμ–΄ μμ
2. λ…Όλ¬Έμ—μ„ μ‚¬μ©ν• λ¨λ“  ν™κ²½μ΄ μ§€μ›λ¨
3. Goal relabelingκ³Ό MPC plannerκ°€ λ…Όλ¬Έκ³Ό μΌμΉν•κ² κµ¬ν„λ¨
4. Grid Searchλ¥Ό ν†µν•΄ ν•μ΄νΌνλΌλ―Έν„°λ¥Ό νƒμƒ‰ν•  μ μμ

**μ ν•μ‚¬ν•­:**
1. λ…Όλ¬Έμ— λ…μ‹λ μ •ν™•ν• ν•μ΄νΌνλΌλ―Έν„° κ°’μ΄ μ—†μ–΄ μΌλ°μ μΈ λ²”μ„ μ‚¬μ©
2. ν™κ²½ λ²„μ „ μ°¨μ΄ (MuJoCo β†’ Gymnasium v5)λ΅ μΈν• λ―Έμ„Έν• μ°¨μ΄ κ°€λ¥

**κ¶μ¥ μ‚¬ν•­:**
1. Grid Searchλ¥Ό ν†µν•΄ κ° ν™κ²½μ— μµμ ν™”λ ν•μ΄νΌνλΌλ―Έν„° νƒμƒ‰
2. λ…Όλ¬Έμ μ‹¤ν— κ²°κ³Όμ™€ λΉ„κµν•μ—¬ ν•μ΄νΌνλΌλ―Έν„° μ΅°μ •
3. μ—¬λ¬ μ‹λ“λ΅ μ‹¤ν—ν•μ—¬ ν†µκ³„μ  μ μμ„± ν™•μΈ

## λ‹¤μ λ‹¨κ³„

1. **ν•μ΄νΌνλΌλ―Έν„° νƒμƒ‰**
   ```bash
   python grid_search.py --env Reacher-v5 --grid-type reduced
   ```

2. **ν•™μµ μ‹¤ν–‰**
   ```bash
   python train_with_curriculum.py --config best_hyperparameters.yaml
   ```

3. **κ²°κ³Ό λΉ„κµ**
   - λ…Όλ¬Έμ μ„±λ¥ μ§€ν‘μ™€ λΉ„κµ
   - ν•„μ”μ‹ ν•μ΄νΌνλΌλ―Έν„° μ¬μ΅°μ •

## μ°Έκ³ 

- λ…Όλ¬Έ: "Temporal Difference Models: Model-Free Deep RL for Model-Based Control" (ICLR 2018)
- arXiv: 1802.09081
- ν„μ¬ κµ¬ν„μ€ λ…Όλ¬Έμ ν•µμ‹¬ μ•„μ΄λ””μ–΄λ¥Ό λ¨λ‘ ν¬ν•¨ν•λ©°, μ¬ν„μ„± μ‹¤ν—μ„ μν–‰ν•  μ μλ” μƒνƒμ…λ‹λ‹¤.

