# Temporal Difference Models (TDM)

ì´ í”„ë¡œì íŠ¸ëŠ” ICLR 2018 ë…¼ë¬¸ "Temporal Difference Models: Model-Free Deep RL for Model-Based Control"ì˜ ì¬í˜„ êµ¬í˜„ì…ë‹ˆë‹¤.

## ë…¼ë¬¸ ì •ë³´

**ì œëª©**: Temporal Difference Models: Model-Free Deep RL for Model-Based Control  
**ì €ì**: Vitchyr Pong, Shixiang Gu, Murtaza Dalal, Sergey Levine  
**í•™íšŒ**: ICLR 2018  
**arXiv**: [1802.09081](https://arxiv.org/abs/1802.09081)

## ê°œìš”

TDM(Temporal Difference Models)ì€ model-freeì™€ model-based ê°•í™”í•™ìŠµì˜ ì¥ì ì„ ê²°í•©í•œ ì•Œê³ ë¦¬ì¦˜ì…ë‹ˆë‹¤:

- **Model-free RLì˜ ì¥ì **: ë†’ì€ ì ê·¼ì  ì„±ëŠ¥, model bias ì—†ìŒ
- **Model-based RLì˜ ì¥ì **: ë†’ì€ ìƒ˜í”Œ íš¨ìœ¨ì„±
- **TDMì˜ í˜ì‹ **: Goal-conditioned value functionì„ í†µí•´ ë‘ ì ‘ê·¼ë²•ì„ ì—°ê²°

### í•µì‹¬ ì•„ì´ë””ì–´

TDMì€ ë‹¤ìŒê³¼ ê°™ì€ goal-conditioned value functionì„ í•™ìŠµí•©ë‹ˆë‹¤:

```
Q(s, a, sg, Ï„) = -||f(s, a, sg, Ï„) - sg||
```

ì—¬ê¸°ì„œ:
- `s`: í˜„ì¬ ìƒíƒœ
- `a`: í–‰ë™
- `sg`: ëª©í‘œ ìƒíƒœ
- `Ï„`: ê³„íš ì§€í‰ì„ (planning horizon)
- `f`: í•™ìŠµëœ ëª¨ë¸ (ìƒíƒœ ì˜ˆì¸¡)

ì´ë¥¼ í†µí•´:
- Ï„=0: 1-step ëª¨ë¸ (model-based)
- Ï„>0: multi-step ì˜ˆì¸¡ (model-free)

## í”„ë¡œì íŠ¸ êµ¬ì¡°

```
.
â”œâ”€â”€ config.yaml                    # ì„¤ì • íŒŒì¼
â”œâ”€â”€ networks.py                    # ì‹ ê²½ë§ êµ¬ì¡° (Actor, Critic)
â”œâ”€â”€ replay_buffer.py               # Replay Buffer ë° Goal Relabeling
â”œâ”€â”€ tdm.py                         # TDM ì•Œê³ ë¦¬ì¦˜ êµ¬í˜„
â”œâ”€â”€ env_wrapper.py                 # í™˜ê²½ ë˜í¼ (goal ì¶”ì¶œ ë“±)
â”œâ”€â”€ mpc_planner.py                 # MPC ê¸°ë°˜ ì •ì±… ì¶”ì¶œ
â”œâ”€â”€ train.py                       # ê¸°ë³¸ í›ˆë ¨ ìŠ¤í¬ë¦½íŠ¸
â”œâ”€â”€ train_with_curriculum.py       # Curriculum Learning í¬í•¨ í›ˆë ¨ ìŠ¤í¬ë¦½íŠ¸ (ê¶Œì¥)
â”œâ”€â”€ evaluate.py                    # í‰ê°€/ì‹œì—° ìŠ¤í¬ë¦½íŠ¸
â”œâ”€â”€ hyperparameter_grid.py         # í•˜ì´í¼íŒŒë¼ë¯¸í„° ê·¸ë¦¬ë“œ ì •ì˜
â”œâ”€â”€ curriculum_learning.py         # Curriculum Learning êµ¬í˜„
â”œâ”€â”€ grid_search.py                 # ë¶„ì‚°ì  Grid Search ìŠ¤í¬ë¦½íŠ¸
â”œâ”€â”€ test_implementation.py         # êµ¬í˜„ ê²€ì¦ í…ŒìŠ¤íŠ¸
â”œâ”€â”€ requirements.txt               # ì˜ì¡´ì„± íŒ¨í‚¤ì§€
â”œâ”€â”€ README.md                      # ì´ íŒŒì¼
â”œâ”€â”€ GRID_SEARCH_GUIDE.md          # Grid Search ì‚¬ìš© ê°€ì´ë“œ
â”œâ”€â”€ PAPER_REPRODUCIBILITY_CHECK.md # ë…¼ë¬¸ ì¬í˜„ì„± ê²€ì¦ ë³´ê³ ì„œ
â””â”€â”€ REPRODUCIBILITY_FINAL_REVIEW.md # ì¬í˜„ì„± ìµœì¢… ê²€í† 
```

## ì„¤ì¹˜

### ë°©ë²• 1: ì•„ë‚˜ì½˜ë‹¤ ì‚¬ìš© (ê¶Œì¥)

```bash
# 1. ì•„ë‚˜ì½˜ë‹¤ í™˜ê²½ ìƒì„±
conda env create -f environment.yml

# 2. í™˜ê²½ í™œì„±í™”
conda activate tdm

# 3. í™˜ê²½ í™•ì¸
python --version  # Python 3.9 ì´ìƒ í™•ì¸
```

### ë°©ë²• 2: pip ì‚¬ìš©

```bash
# Python 3.8 ì´ìƒ ê¶Œì¥
python -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate

# íŒ¨í‚¤ì§€ ì„¤ì¹˜
pip install -r requirements.txt
```

### MuJoCo í™˜ê²½ (í•„ìˆ˜)

ë¡œë´‡ í™˜ê²½(Reacher, Pusher, Ant ë“±)ì„ ì‚¬ìš©í•˜ë ¤ë©´ MuJoCoë¥¼ ì„¤ì¹˜í•´ì•¼ í•©ë‹ˆë‹¤:

```bash
# conda í™˜ê²½ì—ì„œ
conda install -c conda-forge mujoco

# ë˜ëŠ” pipë¡œ
pip install mujoco
```

## ë¹ ë¥¸ ì‹œì‘

### 1. ê¸°ë³¸ í›ˆë ¨

```bash
# ì„¤ì • íŒŒì¼ ìˆ˜ì • (config.yaml)
# í™˜ê²½ ì„ íƒ: Reacher-v5, Pusher-v5, HalfCheetah-v5, Ant-v5

# Curriculum Learning í¬í•¨ í›ˆë ¨ (ê¶Œì¥)
python train_with_curriculum.py

# ë˜ëŠ” ê¸°ë³¸ í›ˆë ¨
python train.py
```

### 2. í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ (Grid Search)

```bash
# íŠ¹ì • í™˜ê²½ì— ëŒ€í•´ ìµœì  í•˜ì´í¼íŒŒë¼ë¯¸í„° ìë™ íƒìƒ‰
python grid_search.py --env Reacher-v5 --grid-type reduced

# ìµœê³  ì„±ëŠ¥ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¡œ í•™ìŠµ
python train_with_curriculum.py --config logs/grid_search_.../best_hyperparameters.yaml
```

### 3. ëª¨ë¸ í‰ê°€

```bash
# í•™ìŠµëœ ëª¨ë¸ í‰ê°€
python evaluate.py --model ./logs/Reacher-v5_.../model_final.pt --episodes 50
```

## ì£¼ìš” ê¸°ëŠ¥

### 1. ë…¼ë¬¸ ì¬í˜„ì„± âœ…
- **TDM ì•Œê³ ë¦¬ì¦˜**: Goal-conditioned Q-function, Bellman equation
- **Goal Relabeling**: Future state sampling, Horizon relabeling
- **MPC Planner**: Direct ë° Optimization ê¸°ë°˜ ì •ì±… ì¶”ì¶œ
- **ì‹¤í—˜ í™˜ê²½**: Reacher, Pusher, HalfCheetah, Ant ëª¨ë‘ ì§€ì›

### 2. í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ğŸ”
- **ë¶„ì‚°ì  Grid Search**: ë©€í‹°í”„ë¡œì„¸ì‹±ì„ í†µí•œ ë³‘ë ¬ íƒìƒ‰
- **ìë™ ìµœì í™”**: ìµœê³  ì„±ëŠ¥ í•˜ì´í¼íŒŒë¼ë¯¸í„° ìë™ ì €ì¥
- **í™˜ê²½ë³„ ê·¸ë¦¬ë“œ**: ê° í™˜ê²½ì— íŠ¹í™”ëœ í•˜ì´í¼íŒŒë¼ë¯¸í„° ë²”ìœ„

### 3. í•™ìŠµ ì•ˆì •í™” ğŸ›¡ï¸
- **Curriculum Learning**: ì‰¬ìš´ ëª©í‘œë¶€í„° ì ì§„ì  í•™ìŠµ
- **Warm-up Period**: ì´ˆê¸° íƒí—˜ ê°•í™”
- **Early Stopping**: ì„±ëŠ¥ ê°œì„  ì—†ì„ ì‹œ ì¡°ê¸° ì¢…ë£Œ
- **Checkpointing**: ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ìë™ ì €ì¥
- **Gradient Clipping**: ê·¸ë˜ë””ì–¸íŠ¸ í­ì£¼ ë°©ì§€
- **Learning Rate Decay**: ìˆ˜ë ´ í›„ í•™ìŠµë¥  ê°ì†Œ

### 4. ëª¨ë‹ˆí„°ë§ ë° ë¶„ì„ ğŸ“Š
- **TensorBoard**: ì‹¤ì‹œê°„ í•™ìŠµ ëª¨ë‹ˆí„°ë§
- **ìë™ í‰ê°€**: ì£¼ê¸°ì  ì„±ëŠ¥ í‰ê°€
- **ê²°ê³¼ ì €ì¥**: ëª¨ë“  ì‹¤í—˜ ê²°ê³¼ ìë™ ì €ì¥

## ì§€ì›í•˜ëŠ” í™˜ê²½

| í™˜ê²½ | ì‘ì—… | Goal Space | íŠ¹ì§• |
|------|------|------------|------|
| **Reacher-v5** | 7-DoF ë¡œë´‡ íŒ”ë¡œ ëª©í‘œ ìœ„ì¹˜ ë„ë‹¬ | End-effector ìœ„ì¹˜ (2D) | ì§ì ‘ì ì¸ ë„ë‹¬ ì‘ì—… |
| **Pusher-v5** | í½ì„ ëª©í‘œ ìœ„ì¹˜ë¡œ ë°€ê¸° | Hand + Puck XY | 2ë‹¨ê³„ ì‘ì—… |
| **HalfCheetah-v5** | ëª©í‘œ ì†ë„ë¡œ ë‹¬ë¦¬ê¸° | ì†ë„ | ì—°ì† ì œì–´ |
| **Ant-v5** | ëª©í‘œ ìœ„ì¹˜ë¡œ ì´ë™ | ìœ„ì¹˜ ë˜ëŠ” ìœ„ì¹˜+ì†ë„ | ë³µì¡í•œ dynamics |

## ì‚¬ìš© ë°©ë²•

### ì„¤ì • íŒŒì¼ (config.yaml)

```yaml
# í™˜ê²½ ì„ íƒ
env:
  name: "Reacher-v5"  # Reacher-v5, Pusher-v5, HalfCheetah-v5, Ant-v5
  max_episode_steps: 50

# TDM í•˜ì´í¼íŒŒë¼ë¯¸í„°
tdm:
  tau_max: 25  # ìµœëŒ€ ê³„íš ì§€í‰ì„ 
  vectorized_supervision: true  # ë²¡í„°í™”ëœ supervision ì‚¬ìš©
  distance_metric: "L1"  # L1 or L2

# í›ˆë ¨ ì„¤ì •
training:
  total_timesteps: 1000000
  learning_rate_actor: 0.0001
  learning_rate_critic: 0.001
  batch_size: 128
  updates_per_step: 10
  polyak: 0.999  # Soft target update
  
  # Curriculum Learning
  use_curriculum: true
  curriculum:
    initial_difficulty: 0.1
    final_difficulty: 1.0
    type: "distance"  # distance or complexity
    schedule: "linear"  # linear, exponential, step
  
  # Warm-up Period
  use_warmup: true
  warmup:
    steps: 10000
    initial_noise_std: 0.5
    final_noise_std: 0.2
  
  # Early Stopping
  patience: 10  # null to disable
  
  # Gradient Clipping
  grad_clip: 1.0  # null to disable
```

### Grid Search ì‚¬ìš©ë²•

```bash
# ì¶•ì†Œ ê·¸ë¦¬ë“œë¡œ ë¹ ë¥¸ íƒìƒ‰ (ê¶Œì¥)
python grid_search.py --env Reacher-v5 --grid-type reduced --workers 4

# ì „ì²´ ê·¸ë¦¬ë“œë¡œ ì™„ì „í•œ íƒìƒ‰
python grid_search.py --env Reacher-v5 --grid-type full

# ìµœëŒ€ ì‹¤í—˜ ìˆ˜ ì œí•œ
python grid_search.py --env Reacher-v5 --grid-type reduced --max-experiments 10
```

ìì„¸í•œ ì‚¬ìš©ë²•ì€ [GRID_SEARCH_GUIDE.md](GRID_SEARCH_GUIDE.md)ë¥¼ ì°¸ì¡°í•˜ì„¸ìš”.

## í•˜ì´í¼íŒŒë¼ë¯¸í„° ê°€ì´ë“œ

### ì¤‘ìš”í•œ í•˜ì´í¼íŒŒë¼ë¯¸í„°

1. **tau_max** (ê³„íš ì§€í‰ì„ )
   - ì‘ì„ìˆ˜ë¡: model-basedì— ê°€ê¹Œì›€, ë¹ ë¥¸ í•™ìŠµ
   - í´ìˆ˜ë¡: model-freeì— ê°€ê¹Œì›€, ë” ê¸´ ê³„íš
   - ê¶Œì¥: 15-25

2. **updates_per_step** (ì—…ë°ì´íŠ¸ ë¹ˆë„)
   - Goal relabeling ë•ë¶„ì— ë†’ì€ ê°’ ê°€ëŠ¥
   - ê¶Œì¥: 5-10

3. **vectorized_supervision**
   - Trueë¡œ ì„¤ì •í•˜ë©´ ì„±ëŠ¥ í–¥ìƒ
   - ê¶Œì¥: True

4. **reward_scale**
   - í™˜ê²½ì— ë”°ë¼ ì¡°ì • í•„ìš”
   - ê¶Œì¥: Grid Searchë¡œ íƒìƒ‰

### Grid Searchë¥¼ í†µí•œ ìµœì í™”

ë…¼ë¬¸ì— ëª…ì‹œëœ ì •í™•í•œ í•˜ì´í¼íŒŒë¼ë¯¸í„° ê°’ì´ ì—†ìœ¼ë¯€ë¡œ, Grid Searchë¥¼ í†µí•´ ìµœì ê°’ì„ íƒìƒ‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

```bash
python grid_search.py --env Reacher-v5 --grid-type reduced
```

## ë…¼ë¬¸ ì¬í˜„ì„±

ì´ í”„ë¡œì íŠ¸ëŠ” ë…¼ë¬¸ì˜ ì‹¤í—˜ í™˜ê²½ê³¼ ë¡œì§ì„ ëª¨ë‘ ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ êµ¬í˜„ë˜ì—ˆìŠµë‹ˆë‹¤:

- âœ… **í•µì‹¬ ì•Œê³ ë¦¬ì¦˜**: TDM Q-function, Goal-conditioned policy, Bellman equation
- âœ… **Goal Relabeling**: Future state sampling, Horizon relabeling
- âœ… **MPC Planner**: Direct ë° Optimization ê¸°ë°˜ ì •ì±… ì¶”ì¶œ
- âœ… **ì‹¤í—˜ í™˜ê²½**: 4ê°œ í™˜ê²½ ëª¨ë‘ ì§€ì›
- âœ… **í•™ìŠµ íŒŒì´í”„ë¼ì¸**: Replay buffer, Target network, Exploration

ìì„¸í•œ ê²€ì¦ ë‚´ìš©ì€ [PAPER_REPRODUCIBILITY_CHECK.md](PAPER_REPRODUCIBILITY_CHECK.md)ë¥¼ ì°¸ì¡°í•˜ì„¸ìš”.

## ì‹¤í—˜ ê²°ê³¼

ë…¼ë¬¸ì—ì„œ ë³´ê³ ëœ ê²°ê³¼ì™€ ìœ ì‚¬í•œ ì„±ëŠ¥ì„ ê¸°ëŒ€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

- **ìƒ˜í”Œ íš¨ìœ¨ì„±**: Model-based RL ìˆ˜ì¤€
- **ìµœì¢… ì„±ëŠ¥**: Model-free RL ìˆ˜ì¤€
- **HER ëŒ€ë¹„**: ë” ë¹ ë¥¸ ìˆ˜ë ´

## ë¬¸ì œ í•´ê²°

### 1. GPU ë©”ëª¨ë¦¬ ë¶€ì¡±
- `batch_size` ì¤„ì´ê¸° (128 â†’ 64)
- `updates_per_step` ì¤„ì´ê¸° (10 â†’ 5)

### 2. í•™ìŠµì´ ëŠë¦¼
- `updates_per_step` ëŠ˜ë¦¬ê¸°
- `vectorized_supervision` í™œì„±í™”
- Grid Searchì˜ `--workers` ìˆ˜ ì¡°ì •

### 3. ì„±ëŠ¥ì´ ë‚®ìŒ
- Grid Searchë¡œ í•˜ì´í¼íŒŒë¼ë¯¸í„° íƒìƒ‰
- `tau_max` ì¡°ì • (15-25)
- `reward_scale` ì¡°ì •
- ë” ì˜¤ë˜ í›ˆë ¨ (`total_timesteps` ì¦ê°€)

### 4. í™˜ê²½ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ
```bash
# MuJoCo ì„¤ì¹˜ í™•ì¸
conda install -c conda-forge mujoco
# ë˜ëŠ”
pip install mujoco
```

## ì°¸ê³  ë¬¸ì„œ

- [GRID_SEARCH_GUIDE.md](GRID_SEARCH_GUIDE.md): Grid Search ìƒì„¸ ê°€ì´ë“œ
- [PAPER_REPRODUCIBILITY_CHECK.md](PAPER_REPRODUCIBILITY_CHECK.md): ë…¼ë¬¸ ì¬í˜„ì„± ê²€ì¦
- [REPRODUCIBILITY_FINAL_REVIEW.md](REPRODUCIBILITY_FINAL_REVIEW.md): ì¬í˜„ì„± ìµœì¢… ê²€í† 

## ì°¸ê³  ë¬¸í—Œ

```bibtex
@inproceedings{pong2018temporal,
  title={Temporal Difference Models: Model-Free Deep RL for Model-Based Control},
  author={Pong, Vitchyr and Gu, Shixiang and Dalal, Murtaza and Levine, Sergey},
  booktitle={International Conference on Learning Representations (ICLR)},
  year={2018}
}
```

## ë¼ì´ì„ ìŠ¤

ì´ êµ¬í˜„ì€ êµìœ¡ ë° ì—°êµ¬ ëª©ì ìœ¼ë¡œ ì œê³µë©ë‹ˆë‹¤.

## ê¸°ì—¬

ë²„ê·¸ ë¦¬í¬íŠ¸ë‚˜ ê°œì„  ì œì•ˆì€ ì´ìŠˆë¡œ ë“±ë¡í•´ ì£¼ì„¸ìš”.

## ê°ì‚¬ì˜ ë§

ì› ë…¼ë¬¸ì˜ ì €ìë“¤ì—ê²Œ ê°ì‚¬ë“œë¦½ë‹ˆë‹¤.
