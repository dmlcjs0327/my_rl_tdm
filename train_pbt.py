"""
Population-based Training (PBT) for TDM
í•™ìŠµ ê³¡ì„ ì„ ë³´ê³  ë™ì ìœ¼ë¡œ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ ì¡°ì •í•˜ëŠ” í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸
"""
import os
import yaml
import numpy as np
import torch
import multiprocessing as mp
import sys
import time
import copy
import random
from datetime import datetime
from torch.utils.tensorboard.writer import SummaryWriter
from typing import Dict, Any

import gymnasium as gym
from tdm import TDM
from env_wrapper import TDMEnvWrapper, GoalSampler
from mpc_planner import TaskSpecificPlanner
from curriculum_learning import CurriculumLearning, WarmUpPeriod
from policy_collapse_detector import PolicyCollapseDetector
from pbt import PopulationBasedTraining, PBTMember
from train_with_curriculum import (
    load_config, set_seed, create_env, evaluate
)
from pbt_monitor import PBTMonitor


def detect_system_resources():
    """
    ì‹œìŠ¤í…œ ìì›ì„ í™•ì¸í•˜ì—¬ ì ì ˆí•œ population_size ì¶”ì²œ
    
    Returns:
        dict: {'cpu_count', 'gpu_count', 'total_memory_gb', 'available_memory_gb'}
    """
    resources = {
        'cpu_count': mp.cpu_count(),
        'gpu_count': 0,
        'total_memory_gb': 0,
        'available_memory_gb': 0,
    }
    
    # GPU í™•ì¸
    if torch.cuda.is_available():
        resources['gpu_count'] = torch.cuda.device_count()
        # GPU ë©”ëª¨ë¦¬ í™•ì¸
        if resources['gpu_count'] > 0:
            gpu_memory_gb = torch.cuda.get_device_properties(0).total_memory / (1024**3)
            resources['gpu_memory_gb'] = gpu_memory_gb
    
    # ë©”ëª¨ë¦¬ í™•ì¸ (psutilì´ ìˆìœ¼ë©´ ì‚¬ìš©)
    try:
        import psutil
        mem = psutil.virtual_memory()
        resources['total_memory_gb'] = mem.total / (1024**3)
        resources['available_memory_gb'] = mem.available / (1024**3)
    except ImportError:
        # psutilì´ ì—†ìœ¼ë©´ ë©”ëª¨ë¦¬ ì •ë³´ëŠ” ìƒëµ
        pass
    
    return resources


def calculate_optimal_population_size(resources, config=None):
    """
    ì‹œìŠ¤í…œ ìì›ì„ ê¸°ë°˜ìœ¼ë¡œ ìµœì ì˜ population_size ê³„ì‚°
    
    Args:
        resources: detect_system_resources()ì˜ ê²°ê³¼
        config: ì„¤ì • ë”•ì…”ë„ˆë¦¬ (í™˜ê²½ë³„ ë©”ëª¨ë¦¬ ìš”êµ¬ì‚¬í•­ í™•ì¸ìš©)
    
    Returns:
        int: ì¶”ì²œë˜ëŠ” population_size
    """
    cpu_count = resources['cpu_count']
    gpu_count = resources['gpu_count']
    available_memory_gb = resources.get('available_memory_gb', 0)
    
    # í™˜ê²½ë³„ ì˜ˆìƒ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ (GB per member)
    env_memory_estimates = {
        'Reacher-v5': 0.5,
        'Pusher-v5': 0.5,
        'HalfCheetah-v5': 1.0,
        'Ant-v5': 1.0,
    }
    
    # í™˜ê²½ ì´ë¦„ í™•ì¸
    env_name = None
    if config:
        env_name = config.get('env', {}).get('name', '')
    
    # í™˜ê²½ë³„ ë©”ëª¨ë¦¬ ìš”êµ¬ì‚¬í•­
    memory_per_member = env_memory_estimates.get(env_name, 0.8)  # ê¸°ë³¸ê°’ 0.8GB
    
    recommended_size = 8  # ê¸°ë³¸ê°’
    
    # GPU ê¸°ë°˜ ê³„ì‚° (ìš°ì„ ìˆœìœ„ 1)
    if gpu_count > 0:
        # GPUê°€ ìˆìœ¼ë©´ GPU ê°œìˆ˜ ê¸°ë°˜
        # ê° GPUë‹¹ 1ê°œ ê°œì²´ (ì•ˆì „ ë§ˆì§„ ê³ ë ¤)
        recommended_size = gpu_count
    else:
        # CPU ê¸°ë°˜ ê³„ì‚°
        # CPU ì½”ì–´ ìˆ˜ì˜ 75% ì‚¬ìš© (ì•ˆì „ ë§ˆì§„)
        # ìµœì†Œ 1ê°œëŠ” ë‚¨ê¹€
        recommended_size = max(1, int(cpu_count * 0.75))
    
    # ë©”ëª¨ë¦¬ ê¸°ë°˜ ì œí•œ (psutilì´ ìˆê³  ë©”ëª¨ë¦¬ ì •ë³´ê°€ ìˆìœ¼ë©´)
    if available_memory_gb > 0:
        # ì‚¬ìš© ê°€ëŠ¥í•œ ë©”ëª¨ë¦¬ì˜ 80% ì‚¬ìš© (ì•ˆì „ ë§ˆì§„)
        max_by_memory = int((available_memory_gb * 0.8) / memory_per_member)
        if max_by_memory < recommended_size:
            recommended_size = max_by_memory
    
    # ìµœì†Œ/ìµœëŒ€ ì œí•œ
    min_population = 4
    max_population = 32
    
    recommended_size = max(min_population, min(recommended_size, max_population))
    
    return recommended_size


def train_single_member_chunk(member_data: tuple) -> Dict[str, Any]:
    """
    ë‹¨ì¼ PBT ê°œì²´ë¥¼ ì§€ì •ëœ ìŠ¤í…ë§Œí¼ í•™ìŠµ (chunk ë‹¨ìœ„, ë©€í‹°í”„ë¡œì„¸ì‹±ìš©)
    
    Args:
        member_data: (member_dict, base_config, log_dir_base, device, steps_to_train, hyperparams)
    """
    member_dict, base_config, log_dir_base, device, steps_to_train, hyperparams = member_data
    
    # PBTMember ê°ì²´ ì¬êµ¬ì„± (ë©€í‹°í”„ë¡œì„¸ì‹±ì„ ìœ„í•´ ë”•ì…”ë„ˆë¦¬ë¡œ ì „ë‹¬)
    from pbt import PBTMember
    member = PBTMember(
        member_id=member_dict['member_id'],
        hyperparameters=hyperparams,
        best_distance=member_dict.get('best_distance', float('inf')),
        current_distance=member_dict.get('current_distance', float('inf')),
        training_steps=member_dict.get('training_steps', 0),
        log_dir=member_dict.get('log_dir', f"pbt_member_{member_dict['member_id']:02d}"),
        model_path=member_dict.get('model_path', '')
    )
    
    return _train_member_internal(member, base_config, None, log_dir_base, device, max_steps=steps_to_train)


def _train_member_internal(member: PBTMember,
                          base_config: Dict[str, Any],
                          pbt: PopulationBasedTraining,
                          log_dir_base: str,
                          device: str,
                          max_steps: int = None) -> Dict[str, Any]:
    """
    ë‹¨ì¼ PBT ê°œì²´ í•™ìŠµ
    
    Args:
        member: PBT ê°œì²´
        base_config: ê¸°ë³¸ ì„¤ì •
        pbt: PBT ì¸ìŠ¤í„´ìŠ¤
        log_dir_base: ë¡œê·¸ ë””ë ‰í† ë¦¬ ê¸°ë³¸ ê²½ë¡œ
        device: ë””ë°”ì´ìŠ¤
    
    Returns:
        í•™ìŠµ ê²°ê³¼
    """
    try:
        # ê°œì²´ ì„¤ì • ê°€ì ¸ì˜¤ê¸°
        if pbt is not None:
            config = pbt.get_member_config(member.member_id)
        else:
            # ë©€í‹°í”„ë¡œì„¸ì‹± í™˜ê²½: í•˜ì´í¼íŒŒë¼ë¯¸í„° ì§ì ‘ ì ìš©
            from hyperparameter_grid import create_config_from_hyperparameters
            config = create_config_from_hyperparameters(base_config, member.hyperparameters)
        
        # ì‹œë“œ ì„¤ì • (ê°œì²´ë³„ë¡œ ë‹¤ë¥¸ ì‹œë“œ)
        set_seed(config['seed'] + member.member_id)
        
        # í™˜ê²½ ìƒì„±
        env = create_env(config)
        
        # ì°¨ì› ì •ë³´
        state_dim = env.observation_space.shape[0]
        action_dim = env.action_space.shape[0]
        action_range = (env.action_space.low, env.action_space.high)
        goal_dim = env.goal_dim
        
        # TDM Agent ìƒì„±
        tdm = TDM(state_dim, action_dim, goal_dim, action_range, config, device)
        
        # Planner ìƒì„±
        planner = TaskSpecificPlanner(tdm, config, config['env']['name'],
                                     config['task']['locomotion_task_type'])
        
        # Goal Sampler
        goal_sampler = GoalSampler(config['task']['locomotion_task_type'],
                                  config['env']['name'])
        
        # Curriculum Learning
        use_curriculum = config['training'].get('use_curriculum', False)
        curriculum = None
        if use_curriculum:
            curriculum_config = config['training'].get('curriculum', {})
            curriculum = CurriculumLearning(
                goal_sampler,
                initial_difficulty=curriculum_config.get('initial_difficulty', 0.1),
                final_difficulty=curriculum_config.get('final_difficulty', 1.0),
                curriculum_type=curriculum_config.get('type', 'distance'),
                schedule=curriculum_config.get('schedule', 'linear')
            )
        
        # Warm-up Period
        use_warmup = config['training'].get('use_warmup', False)
        warmup = None
        if use_warmup:
            warmup_config = config['training'].get('warmup', {})
            warmup = WarmUpPeriod(
                warmup_steps=warmup_config.get('steps', 10000),
                initial_noise_std=warmup_config.get('initial_noise_std', 0.5),
                final_noise_std=warmup_config.get('final_noise_std', 0.2),
                initial_lr_multiplier=warmup_config.get('initial_lr_multiplier', 0.1),
                final_lr_multiplier=warmup_config.get('final_lr_multiplier', 1.0)
            )
        
        # Policy Collapse Detector
        use_collapse_detection = config['training'].get('detect_policy_collapse', False)
        collapse_detector = None
        if use_collapse_detection:
            collapse_config = config['training'].get('collapse_detection', {})
            collapse_detector = PolicyCollapseDetector(
                window_size=collapse_config.get('window_size', 5),
                collapse_threshold=collapse_config.get('collapse_threshold', 0.3),
                min_evaluations=collapse_config.get('min_evaluations', 3),
                stability_threshold=collapse_config.get('stability_threshold', 0.5)
            )
        
        # ë¡œê·¸ ë””ë ‰í† ë¦¬
        member_log_dir = os.path.join(log_dir_base, member.log_dir)
        os.makedirs(member_log_dir, exist_ok=True)
        
        # TensorBoard
        if config['logging']['tensorboard']:
            writer = SummaryWriter(member_log_dir)
        
        # ì„¤ì • ì €ì¥
        with open(os.path.join(member_log_dir, 'config.yaml'), 'w', encoding='utf-8') as f:
            yaml.dump(config, f, allow_unicode=True)
        
        # í•™ìŠµ ë£¨í”„
        start_steps = member.training_steps  # ì´ì „ í•™ìŠµ ìŠ¤í…ë¶€í„° ì´ì–´ì„œ
        total_steps = start_steps
        target_steps = start_steps + max_steps if max_steps else config['training']['total_timesteps']
        episode = 0
        episode_reward = 0
        episode_length = 0
        
        best_eval_distance = member.best_distance if member.best_distance != float('inf') else float('inf')
        best_model_path = os.path.join(member_log_dir, 'model_best.pt')
        patience = config['training'].get('patience', None)
        patience_counter = 0
        
        # ì´ì „ ëª¨ë¸ ë¡œë“œ (ìˆëŠ” ê²½ìš°)
        if member.model_path and os.path.exists(member.model_path):
            tdm.load(member.model_path)
            print(f"  [Member {member.member_id:02d}] Loaded previous model from {member.model_path}")
        
        obs, info = env.reset()
        initial_state = obs.copy()
        
        # Curriculum learning: ì´ˆê¸° ëª©í‘œ ìƒ˜í”Œë§
        goal = env.get_goal()
        if curriculum is not None:
            # ì§„í–‰ë„ ë³µì›
            progress = start_steps / config['training']['total_timesteps']
            curriculum.update_progress(progress)
            curriculum_goal = curriculum.sample_goal(initial_state)
            env.current_goal = curriculum_goal
            goal = curriculum_goal
        
        while total_steps < target_steps and total_steps < config['training']['total_timesteps']:
            # Warm-up ì—…ë°ì´íŠ¸
            if warmup:
                warmup.update_step(total_steps)
                tdm.noise_std = warmup.get_noise_std(config['training']['noise_std'])
                if warmup.is_warmup():
                    lr_multiplier = warmup.get_lr_multiplier()
                    for param_group in tdm.actor_optimizer.param_groups:
                        param_group['lr'] = config['training']['learning_rate_actor'] * lr_multiplier
                    for param_group in tdm.critic_optimizer.param_groups:
                        param_group['lr'] = config['training']['learning_rate_critic'] * lr_multiplier
            
            # í–‰ë™ ì„ íƒ
            tau = config['tdm']['tau_max']
            action = planner.select_action(obs, goal, tau)
            
            # Safety check: ensure action is valid before adding noise
            if np.isnan(action).any() or np.isinf(action).any():
                print(f"  âš ï¸  Warning: Invalid action detected at step {total_steps}, using zero action")
                action = np.zeros(action_dim)
            
            noise = np.random.normal(0, tdm.noise_std, size=action_dim)
            # Safety check: ensure noise is valid
            if np.isnan(noise).any() or np.isinf(noise).any():
                noise = np.zeros_like(noise)
            action = np.clip(action + noise, action_range[0], action_range[1])
            
            # Final safety check before environment step
            if np.isnan(action).any() or np.isinf(action).any():
                print(f"  âš ï¸  Warning: Invalid action after noise at step {total_steps}, using zero action")
                action = np.zeros(action_dim)
                action = np.clip(action, action_range[0], action_range[1])
            
            # í™˜ê²½ ìŠ¤í…
            next_obs, reward, terminated, truncated, info = env.step(action)
            
            # Safety check: ensure observation is valid
            if np.isnan(next_obs).any() or np.isinf(next_obs).any():
                print(f"  âš ï¸  Warning: Invalid observation detected at step {total_steps}")
                next_obs = np.nan_to_num(next_obs, nan=0.0, posinf=0.0, neginf=0.0)
            
            # Replay Bufferì— ì €ì¥
            done = terminated or truncated
            tdm.replay_buffer.add(obs, action, next_obs, reward, done, goal)
            
            obs = next_obs
            episode_reward += reward
            episode_length += 1
            total_steps += 1
            
            # í•™ìŠµ (Replay Bufferì— ì¶©ë¶„í•œ ìƒ˜í”Œì´ ìˆì„ ë•Œë§Œ)
            if total_steps >= config['training']['batch_size'] and tdm.replay_buffer.size >= config['training']['batch_size']:
                for _ in range(config['training']['updates_per_step']):
                    train_info = tdm.train_step()
                    
                    if train_info is not None and config['logging']['tensorboard']:
                        if total_steps % config['logging']['log_frequency'] == 0:
                            # Safety check: ensure loss values are valid before logging
                            critic_loss = train_info.get('critic_loss', 0.0)
                            actor_loss = train_info.get('actor_loss', 0.0)
                            if not (np.isnan(critic_loss) or np.isinf(critic_loss)):
                                writer.add_scalar('train/critic_loss', critic_loss, total_steps)
                            if not (np.isnan(actor_loss) or np.isinf(actor_loss)):
                                writer.add_scalar('train/actor_loss', actor_loss, total_steps)
            
            # ë…¸ì´ì¦ˆ ê°ì†Œ
            if not warmup or not warmup.is_warmup():
                if total_steps % 1000 == 0:
                    tdm.noise_std = max(tdm.noise_std * config['training']['noise_decay'],
                                       config['training'].get('min_noise_std', 0.01))
            
            # ì—í”¼ì†Œë“œ ì¢…ë£Œ
            done = terminated or truncated
            if done or episode_length >= config['env']['max_episode_steps']:
                if config['logging']['tensorboard']:
                    writer.add_scalar('train/episode_reward', episode_reward, episode)
                    writer.add_scalar('train/episode_length', episode_length, episode)
                    writer.add_scalar('train/noise_std', tdm.noise_std, total_steps)
                
                # í™˜ê²½ ë¦¬ì…‹
                obs, info = env.reset()
                # Safety check: ensure observation is valid after reset
                if np.isnan(obs).any() or np.isinf(obs).any():
                    print(f"  âš ï¸  Warning: Invalid observation after reset at episode {episode}")
                    obs = np.nan_to_num(obs, nan=0.0, posinf=0.0, neginf=0.0)
                initial_state = obs.copy()
                
                # Curriculum learning: ìƒˆë¡œìš´ ëª©í‘œ ìƒ˜í”Œë§
                goal = env.get_goal()
                if curriculum is not None:
                    progress = total_steps / config['training']['total_timesteps']
                    curriculum.update_progress(progress)
                    curriculum_goal = curriculum.sample_goal(initial_state)
                    env.current_goal = curriculum_goal
                    goal = curriculum_goal
                
                episode_reward = 0
                episode_length = 0
                episode += 1
            
            # ì£¼ê¸°ì  í‰ê°€
            if total_steps % config['training']['eval_frequency'] == 0:
                # ìƒíƒœ ì—…ë°ì´íŠ¸: í‰ê°€ ì¤‘
                monitor = PBTMonitor(log_dir_base)
                monitor.update_member_status(member.member_id, {
                    'training_steps': total_steps,
                    'best_distance': best_eval_distance,
                    'current_distance': current_distance if 'current_distance' in locals() else float('inf'),
                    'state': 'evaluating'
                })
                
                # í‰ê°€ ì‹œì‘ ì¶œë ¥
                progress_pct = (total_steps / config['training']['total_timesteps'] * 100) if config['training']['total_timesteps'] > 0 else 0
                print(f"[Member {member.member_id:02d}] í‰ê°€ ì¤‘... (Steps: {total_steps}/{config['training']['total_timesteps']}, {progress_pct:.1f}%, Episode: {episode})")
                
                eval_results = evaluate(env, tdm, planner,
                                       config['training']['eval_episodes'],
                                       config)
                
                current_distance = eval_results['mean_distance']
                is_best = current_distance < best_eval_distance
                
                # í‰ê°€ ê²°ê³¼ ì¶œë ¥
                print(f"[Member {member.member_id:02d}] í‰ê°€ ì™„ë£Œ:")
                print(f"  í˜„ì¬ ê±°ë¦¬: {current_distance:.4f} | ìµœê³  ê±°ë¦¬: {best_eval_distance:.4f} | ì„±ê³µë¥ : {eval_results['success_rate']:.2%}")
                
                # ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì €ì¥
                if is_best:
                    best_eval_distance = current_distance
                    patience_counter = 0
                    tdm.save(best_model_path)
                    print(f"  âœ… ìµœê³  ì„±ëŠ¥ ë‹¬ì„±! ëª¨ë¸ ì €ì¥ë¨ (ê°œì„ : {best_eval_distance:.4f})")
                else:
                    patience_counter += 1
                    improvement = best_eval_distance - current_distance
                    print(f"  ğŸ“Š ì„±ëŠ¥: {improvement:+.4f} (patience: {patience_counter}/{patience if patience else 'N/A'})")
                
                # Policy Collapse Detection
                collapsed = False
                if collapse_detector:
                    collapse_info = collapse_detector.update(
                        eval_results['mean_distance'],
                        eval_results['success_rate']
                    )
                    if collapse_info['is_collapsed']:
                        collapsed = True
                        print(f"  âš ï¸  Policy Collapse ê°ì§€: {collapse_info.get('reason', 'Performance degradation')}")
                
                # ìƒíƒœ ì—…ë°ì´íŠ¸: í•™ìŠµ ì¤‘
                monitor = PBTMonitor(log_dir_base)
                monitor.update_member_status(member.member_id, {
                    'training_steps': total_steps,
                    'best_distance': best_eval_distance,
                    'current_distance': current_distance,
                    'state': 'training',
                    'episode': episode,
                    'early_stopped': False,
                    'early_stop_reason': None
                })
                
                # PBT ì—…ë°ì´íŠ¸ëŠ” ë©”ì¸ í”„ë¡œì„¸ìŠ¤ì—ì„œë§Œ ìˆ˜í–‰ (ë©€í‹°í”„ë¡œì„¸ì‹± í™˜ê²½ì—ì„œëŠ” ìƒëµ)
                # í‰ê°€ ë°ì´í„°ëŠ” ë°˜í™˜ê°’ì— í¬í•¨
                
                # Early stopping
                early_stop_reason = None
                if patience is not None and patience_counter >= patience:
                    early_stop_reason = f"Early stopping: No improvement for {patience} evaluations (patience exceeded)"
                    print(f"\nâš ï¸  [Member {member.member_id:02d}] {early_stop_reason}")
                    print(f"   ìµœì¢… Steps: {total_steps}, ìµœê³  ê±°ë¦¬: {best_eval_distance:.4f}")
                    if best_model_path and os.path.exists(best_model_path):
                        tdm.load(best_model_path)
                        print(f"   ìµœê³  ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
                    break
                
                if collapsed:
                    early_stop_reason = f"Policy collapse detected: {collapse_info.get('reason', 'Performance degradation')}"
                    print(f"\nâš ï¸  [Member {member.member_id:02d}] {early_stop_reason}")
                    print(f"   ìµœì¢… Steps: {total_steps}, ìµœê³  ê±°ë¦¬: {best_eval_distance:.4f}")
                    if best_model_path and os.path.exists(best_model_path):
                        tdm.load(best_model_path)
                        print(f"   ìµœê³  ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
                    break
                
                if config['logging']['tensorboard']:
                    writer.add_scalar('eval/mean_distance', eval_results['mean_distance'], total_steps)
                    writer.add_scalar('eval/success_rate', eval_results['success_rate'], total_steps)
                    writer.add_scalar('eval/mean_length', eval_results['mean_length'], total_steps)
                    writer.add_scalar('eval/best_distance', best_eval_distance, total_steps)
            
            # ëª¨ë¸ ì €ì¥
            if total_steps % config['logging']['save_frequency'] == 0:
                model_path = os.path.join(member_log_dir, f'model_{total_steps}.pt')
                tdm.save(model_path)
                print(f"[Member {member.member_id:02d}] ì²´í¬í¬ì¸íŠ¸ ì €ì¥: {total_steps} steps")
        
        # ìµœì¢… ì €ì¥
        final_model_path = os.path.join(member_log_dir, 'model_final.pt')
        if best_model_path and os.path.exists(best_model_path):
            import shutil
            shutil.copy(best_model_path, final_model_path)
        else:
            tdm.save(final_model_path)
        
        env.close()
        if config['logging']['tensorboard']:
            writer.close()
        
        # ì¡°ê¸° ì¢…ë£Œ ì—¬ë¶€ í™•ì¸
        early_stopped = 'early_stop_reason' in locals() and early_stop_reason is not None
        
        return {
            'member_id': member.member_id,
            'best_distance': best_eval_distance,
            'current_distance': current_distance if 'current_distance' in locals() else best_eval_distance,
            'training_steps': total_steps,
            'log_dir': member_log_dir,
            'model_path': final_model_path,
            'collapsed': collapsed if 'collapsed' in locals() else False,
            'early_stopped': early_stopped,
            'early_stop_reason': early_stop_reason if early_stopped else None,
            'eval_data': {
                'mean_distance': eval_results['mean_distance'] if 'eval_results' in locals() else best_eval_distance,
                'std_distance': eval_results.get('std_distance', 0.0) if 'eval_results' in locals() else 0.0,
                'success_rate': eval_results.get('success_rate', 0.0) if 'eval_results' in locals() else 0.0,
                'mean_length': eval_results.get('mean_length', 0.0) if 'eval_results' in locals() else 0.0,
                'mean_reward': eval_results.get('mean_reward', 0.0) if 'eval_results' in locals() else 0.0,
            } if 'eval_results' in locals() else None
        }
    
    except Exception as e:
        import traceback
        traceback.print_exc()
        
        # ì—ëŸ¬ ìƒíƒœ ì—…ë°ì´íŠ¸
        monitor = PBTMonitor(log_dir_base)
        monitor.update_member_status(member.member_id, {
            'training_steps': member.training_steps,
            'best_distance': member.best_distance,
            'current_distance': member.current_distance,
            'state': 'error',
            'error': str(e)
        })
        
        return {
            'member_id': member.member_id,
            'best_distance': float('inf'),
            'current_distance': float('inf'),
            'training_steps': member.training_steps,
            'log_dir': '',
            'model_path': '',
            'error': str(e),
            'collapsed': False
        }


def train_pbt(config_path: str = 'config.yaml'):
    """
    Population-based Training ë©”ì¸ í•¨ìˆ˜
    """
    # ì„¤ì • ë¡œë“œ
    base_config = load_config(config_path)
    
    # PBT ì„¤ì •
    pbt_config = base_config.get('pbt', {})
    population_size_config = pbt_config.get('population_size', 'auto')  # ê¸°ë³¸ê°’: auto
    
    # population_sizeê°€ "auto" ë˜ëŠ” nullì´ë©´ ìë™ ê²°ì •
    if population_size_config is None or (isinstance(population_size_config, str) and population_size_config.lower() == 'auto'):
        print(f"\n{'='*60}")
        print("ğŸ” ì‹œìŠ¤í…œ ìì› í™•ì¸ ì¤‘...")
        print(f"{'='*60}")
        
        resources = detect_system_resources()
        print(f"  CPU ì½”ì–´: {resources['cpu_count']}ê°œ")
        if resources['gpu_count'] > 0:
            print(f"  GPU: {resources['gpu_count']}ê°œ")
            if 'gpu_memory_gb' in resources:
                print(f"  GPU ë©”ëª¨ë¦¬: {resources['gpu_memory_gb']:.1f}GB")
        if resources['total_memory_gb'] > 0:
            print(f"  ì´ ë©”ëª¨ë¦¬: {resources['total_memory_gb']:.1f}GB")
            print(f"  ì‚¬ìš© ê°€ëŠ¥ ë©”ëª¨ë¦¬: {resources['available_memory_gb']:.1f}GB")
        
        population_size = calculate_optimal_population_size(resources, base_config)
        print(f"\nâœ… ìë™ ê²°ì •ëœ Population Size: {population_size}ê°œ")
        print(f"{'='*60}\n")
    else:
        population_size = population_size_config
    
    exploit_frequency = pbt_config.get('exploit_frequency', 10000)
    exploit_threshold = pbt_config.get('exploit_threshold', 0.25)
    explore_perturbation = pbt_config.get('explore_perturbation', 0.2)
    
    # í•˜ì´í¼íŒŒë¼ë¯¸í„° ë²”ìœ„
    hyperparameter_ranges = pbt_config.get('hyperparameter_ranges', None)
    
    # PBT ì´ˆê¸°í™”
    pbt = PopulationBasedTraining(
        base_config=base_config,
        population_size=population_size,
        exploit_frequency=exploit_frequency,
        exploit_threshold=exploit_threshold,
        explore_perturbation=explore_perturbation,
        hyperparameter_ranges=hyperparameter_ranges
    )
    
    # ë¡œê·¸ ë””ë ‰í† ë¦¬
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    log_dir_base = os.path.join(base_config['logging']['log_dir'],
                                f"pbt_{base_config['env']['name']}_{timestamp}")
    os.makedirs(log_dir_base, exist_ok=True)
    
    # ë””ë°”ì´ìŠ¤ ì •ë³´ ìƒì„¸ í™•ì¸ ë° ì¶œë ¥
    cuda_available = torch.cuda.is_available()
    if cuda_available:
        gpu_count = torch.cuda.device_count()
        gpu_name = torch.cuda.get_device_name(0) if gpu_count > 0 else "Unknown"
        device = torch.device('cuda')
        use_gpu = True
    else:
        gpu_count = 0
        gpu_name = None
        device = torch.device('cpu')
        use_gpu = False
    
    print(f"\n{'='*60}")
    print(f"Population-based Training (PBT)")
    print(f"Environment: {base_config['env']['name']}")
    print(f"Device: {device} ({'GPU' if use_gpu else 'CPU'} ì‚¬ìš©)")
    if use_gpu:
        print(f"GPU Count: {gpu_count}")
        print(f"GPU Name: {gpu_name}")
    else:
        print(f"âš ï¸  CUDA ì‚¬ìš© ë¶ˆê°€ - CPUë¡œ í•™ìŠµí•©ë‹ˆë‹¤")
        print(f"   torch.cuda.is_available() = {cuda_available}")
        # PyTorch ë²„ì „ ì •ë³´ í™•ì¸
        try:
            if hasattr(torch.version, 'cuda') and torch.version.cuda:
                print(f"   PyTorchëŠ” CUDAë¥¼ ì§€ì›í•˜ì§€ë§Œ, CUDA ë“œë¼ì´ë²„ê°€ ì¸ì‹ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤")
                print(f"   PyTorch CUDA ë²„ì „: {torch.version.cuda}")
            else:
                print(f"   PyTorchê°€ CPU ì „ìš© ë²„ì „ìœ¼ë¡œ ì„¤ì¹˜ë˜ì–´ ìˆìŠµë‹ˆë‹¤")
        except:
            pass
        print(f"   PyTorch ë²„ì „: {torch.__version__}")
        print(f"   ğŸ’¡ GPUë¥¼ ì‚¬ìš©í•˜ë ¤ë©´ CUDA ì§€ì› PyTorchë¥¼ ì„¤ì¹˜í•˜ì„¸ìš”:")
        print(f"      conda install pytorch torchvision torchaudio pytorch-cuda=11.8 -c pytorch -c nvidia")
    print(f"Population Size: {population_size}")
    print(f"Exploit Frequency: {exploit_frequency} steps")
    print(f"Log Directory: {log_dir_base}")
    print(f"{'='*60}\n")
    
    # ì´ˆê¸° ê°œì²´êµ° í•˜ì´í¼íŒŒë¼ë¯¸í„° ì¶œë ¥
    print("Initial Population Hyperparameters:")
    print(f"{'='*60}")
    for member in pbt.population:
        print(f"Member {member.member_id:02d}:")
        for key, value in member.hyperparameters.items():
            print(f"  {key}: {value}")
    print(f"{'='*60}\n")
    
    # í•™ìŠµ ë£¨í”„
    max_generations = pbt_config.get('max_generations', 100)
    total_timesteps = base_config['training']['total_timesteps']
    training_chunk = pbt_config.get('training_chunk', 5000)  # í•œ ë²ˆì— í•™ìŠµí•  ìŠ¤í… ìˆ˜
    
    print(f"\nStarting PBT training...")
    print(f"Total timesteps per member: {total_timesteps}")
    print(f"Training chunk: {training_chunk} steps")
    print(f"Exploit frequency: {exploit_frequency} steps\n")
    
    # ë³‘ë ¬ ì‹¤í–‰ ì„¤ì •
    num_workers = pbt_config.get('num_workers', None)
    if num_workers is None:
        num_workers = min(population_size, max(1, mp.cpu_count() - 1))
    
    use_parallel = pbt_config.get('use_parallel', True) and num_workers > 1
    
    if use_parallel:
        print(f"Using parallel execution with {num_workers} workers")
    else:
        print(f"Using sequential execution")
    
    # ëª¨ë‹ˆí„° ì´ˆê¸°í™”
    monitor = PBTMonitor(log_dir_base)
    monitor_interval = pbt_config.get('monitor_interval', 5)  # ëª‡ ì´ˆë§ˆë‹¤ ìƒíƒœ ì¶œë ¥
    last_monitor_time = time.time()
    
    print(f"\nğŸ’¡ Tip: í•™ìŠµ ì§„í–‰ ìƒí™©ì€ {monitor_interval}ì´ˆë§ˆë‹¤ ìë™ìœ¼ë¡œ ì¶œë ¥ë©ë‹ˆë‹¤.")
    print(f"   TensorBoardë¡œë„ ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§ ê°€ëŠ¥: tensorboard --logdir {log_dir_base}\n")
    
    iteration = 0
    while True:
        iteration += 1
        # ëª¨ë“  ê°œì²´ê°€ ì™„ë£Œë˜ì—ˆëŠ”ì§€ í™•ì¸
        if all(m.training_steps >= total_timesteps for m in pbt.population):
            print("\nAll members completed training!")
            break
        
        # í•™ìŠµí•  ê°œì²´ ì„ íƒ (ì¡°ê¸° ì¢…ë£Œëœ ê°œì²´ëŠ” ì œì™¸)
        members_to_train = [m for m in pbt.population 
                          if m.training_steps < total_timesteps and not m.early_stopped]
        
        # ì¡°ê¸° ì¢…ë£Œëœ ê°œì²´ ì¬ì‹œì‘ (ìƒˆë¡œìš´ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¡œ)
        early_stopped_members = [m for m in pbt.population 
                                if m.early_stopped and m.training_steps < total_timesteps]
        
        if early_stopped_members:
            print(f"\nğŸ”„ Restarting {len(early_stopped_members)} early-stopped members using PBT-style exploit/explore...")
            
            # PBT ì›ë…¼ë¬¸ ë°©ì‹: Truncation Selection
            # ì„±ëŠ¥ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬ (ê±°ë¦¬ê°€ ì‘ì„ìˆ˜ë¡ ì¢‹ìŒ)
            sorted_population = sorted(pbt.population, key=lambda x: x.best_distance)
            
            # ì„±ê³µí•œ ê°œì²´ë“¤ (ì¡°ê¸° ì¢…ë£Œë˜ì§€ ì•Šê³  ì¶©ë¶„íˆ í•™ìŠµëœ ê°œì²´ë“¤)
            successful_members = pbt.get_successful_members(min_training_steps=1000)
            
            # ìƒìœ„ ì„±ëŠ¥ ê°œì²´ë“¤ (exploit ëŒ€ìƒ) - í•˜ìœ„ 20% ì œì™¸í•œ ë‚˜ë¨¸ì§€
            exploit_threshold_idx = max(1, int(len(sorted_population) * (1 - pbt.exploit_threshold)))
            top_members = sorted_population[exploit_threshold_idx:] if exploit_threshold_idx < len(sorted_population) else sorted_population
            
            for member in early_stopped_members:
                # ì¡°ê¸° ì¢…ë£Œ ì‚¬ìœ ì— ë”°ë¼ ë‹¤ë¥¸ ì „ëµ ì‚¬ìš©
                stop_reason = member.early_stop_reason or ""
                source_member = None
                
                # PBT ì›ë…¼ë¬¸ ë°©ì‹: Exploit (ìƒìœ„ ê°œì²´ ì¤‘ ì„ íƒ) + Explore (ë³€í˜•)
                if top_members:
                    # ì„±ëŠ¥ ê¸°ë°˜ ê°€ì¤‘ì¹˜ ì„ íƒ (ë” ì¢‹ì€ ê°œì²´ê°€ ì„ íƒë  í™•ë¥  ë†’ìŒ)
                    # ë˜ëŠ” ë‹¨ìˆœ ëœë¤ ì„ íƒ (ì›ë…¼ë¬¸ì—ì„œëŠ” ëœë¤)
                    source_member = random.choice(top_members)
                    new_hyperparams = copy.deepcopy(source_member.hyperparameters)
                    
                    # ì¡°ê¸° ì¢…ë£Œ ì‚¬ìœ ì— ë”°ë¼ ë³€í˜• ì •ë„ ì¡°ì ˆ
                    if "Policy collapse" in stop_reason or "collapse" in stop_reason.lower():
                        # Policy Collapse: í° ë³€í˜• (2ë°°) - ì‹¤íŒ¨í•œ ì˜ì—­ì„ ë²—ì–´ë‚˜ê¸° ìœ„í•´
                        perturbation_factor = 2.0
                        strategy_type = "Policy collapse recovery"
                    elif "Early stopping" in stop_reason or "patience" in stop_reason.lower():
                        # Early Stopping: ì‘ì€ ë³€í˜• (1ë°°) - ì•½ê°„ì˜ íƒìƒ‰ë§Œ
                        perturbation_factor = 1.0
                        strategy_type = "Early stopping recovery"
                    else:
                        # ì•Œ ìˆ˜ ì—†ëŠ” ì‚¬ìœ : ì¤‘ê°„ ë³€í˜• (1.5ë°°)
                        perturbation_factor = 1.5
                        strategy_type = "Unknown reason recovery"
                    
                    # Explore: ë³€í˜• ì ìš©
                    for key in new_hyperparams:
                        if key in pbt.hyperparameter_ranges:
                            new_hyperparams[key] = pbt._perturb_hyperparameter(
                                new_hyperparams[key], key, perturbation_factor=perturbation_factor
                            )
                    
                    strategy = f"{strategy_type}: Exploit from member {source_member.member_id:02d} (perturbation={perturbation_factor}x)"
                else:
                    # ìƒìœ„ ê°œì²´ê°€ ì—†ìœ¼ë©´ (ëª¨ë‘ ì¡°ê¸° ì¢…ë£Œëœ ê²½ìš°) ì™„ì „íˆ ìƒˆë¡œìš´ ëœë¤
                    new_hyperparams = pbt._sample_random_hyperparameters()
                    strategy = "Complete random restart (no successful members available)"
                
                # ê°œì²´ ì¬ì‹œì‘
                member.hyperparameters = new_hyperparams
                member.early_stopped = False
                member.early_stop_reason = None
                member.training_steps = 0  # ì²˜ìŒë¶€í„° ì¬ì‹œì‘
                member.best_distance = float('inf')
                member.current_distance = float('inf')
                member.model_path = ""
                
                print(f"  Member {member.member_id:02d}: {strategy}")
                print(f"    New hyperparams: {new_hyperparams}")
                
                # í•˜ì´í¼íŒŒë¼ë¯¸í„° ë³€í™” ì´ë ¥ ì €ì¥
                if member.hyperparameter_history is None:
                    member.hyperparameter_history = []
                member.hyperparameter_history.append({
                    'generation': pbt.generation,
                    'training_steps': 0,
                    'hyperparameters': copy.deepcopy(new_hyperparams),
                    'strategy': strategy,
                    'reason': stop_reason,
                    'exploited_from': source_member.member_id if source_member is not None else None
                })
                
                members_to_train.append(member)
        
        if not members_to_train:
            break
        
        # ë³‘ë ¬ ë˜ëŠ” ìˆœì°¨ ì‹¤í–‰
        if use_parallel and len(members_to_train) > 1:
            # ë³‘ë ¬ ì‹¤í–‰
            training_args = []
            for member in members_to_train:
                steps_to_train = min(training_chunk, total_timesteps - member.training_steps)
                # ë©€í‹°í”„ë¡œì„¸ì‹±ì„ ìœ„í•´ ë”•ì…”ë„ˆë¦¬ë¡œ ì§ë ¬í™”
                member_dict = {
                    'member_id': member.member_id,
                    'best_distance': member.best_distance,
                    'current_distance': member.current_distance,
                    'training_steps': member.training_steps,
                    'log_dir': member.log_dir,
                    'model_path': member.model_path
                }
                training_args.append((
                    member_dict, base_config, log_dir_base, str(device), 
                    steps_to_train, member.hyperparameters
                ))
            
            # ë©€í‹°í”„ë¡œì„¸ì‹±ìœ¼ë¡œ ë³‘ë ¬ ì‹¤í–‰
            print(f"\nğŸ”„ Starting parallel training for {len(members_to_train)} members...")
            start_time = time.time()
            
            # ë¹„ë™ê¸° ì‹¤í–‰ìœ¼ë¡œ ì§„í–‰ ìƒí™© ëª¨ë‹ˆí„°ë§ ê°€ëŠ¥í•˜ê²Œ
            with mp.Pool(processes=min(num_workers, len(members_to_train))) as pool:
                # ë¹„ë™ê¸° ì‹¤í–‰
                async_results = pool.map_async(train_single_member_chunk, training_args)
                
                # ì§„í–‰ ìƒí™© ëª¨ë‹ˆí„°ë§ (ì£¼ê¸°ì ìœ¼ë¡œ ìƒíƒœ ì¶œë ¥)
                while not async_results.ready():
                    current_time = time.time()
                    if current_time - last_monitor_time >= monitor_interval:
                        monitor.print_status(population_size, total_timesteps)
                        last_monitor_time = current_time
                    time.sleep(1)  # 1ì´ˆë§ˆë‹¤ ì²´í¬
                
                results = async_results.get()
            
            elapsed_time = time.time() - start_time
            print(f"âœ… Parallel training completed in {elapsed_time:.1f} seconds\n")
            
            # ê²°ê³¼ ì—…ë°ì´íŠ¸
            print(f"\nğŸ“Š ë³‘ë ¬ í•™ìŠµ ì™„ë£Œ - ê²°ê³¼ ìš”ì•½:")
            for result in results:
                if 'error' not in result:
                    eval_data = result.get('eval_data')
                    member = pbt.population[result['member_id']]
                    progress_pct = (result['training_steps'] / total_timesteps * 100) if total_timesteps > 0 else 0
                    
                    # ì¡°ê¸° ì¢…ë£Œ ì •ë³´ ì—…ë°ì´íŠ¸
                    if result.get('early_stopped', False):
                        member.early_stopped = True
                        member.early_stop_reason = result.get('early_stop_reason', 'Unknown')
                        print(f"  âš ï¸  Member {result['member_id']:02d}: ì¡°ê¸° ì¢…ë£Œ")
                        print(f"     ì‚¬ìœ : {member.early_stop_reason}")
                        print(f"     Steps: {result['training_steps']}/{total_timesteps} ({progress_pct:.1f}%)")
                        print(f"     ìµœê³  ê±°ë¦¬: {result['best_distance']:.4f}")
                        print(f"     ë‹¤ìŒ ë°˜ë³µì—ì„œ ì¬ì‹œì‘ ì˜ˆì •")
                    else:
                        print(f"  âœ… Member {result['member_id']:02d}: í•™ìŠµ ì§„í–‰ ì¤‘")
                        print(f"     Steps: {result['training_steps']}/{total_timesteps} ({progress_pct:.1f}%)")
                        print(f"     ìµœê³  ê±°ë¦¬: {result['best_distance']:.4f} | í˜„ì¬ ê±°ë¦¬: {result['current_distance']:.4f}")
                        if eval_data:
                            print(f"     ì„±ê³µë¥ : {eval_data.get('success_rate', 0):.2%}")
                    
                    pbt.update_member(
                        result['member_id'],
                        result['best_distance'],
                        result['current_distance'],
                        result['training_steps'],
                        result['log_dir'],
                        result['model_path'],
                        eval_data=eval_data
                    )
                else:
                    print(f"  âŒ Member {result['member_id']:02d}: ì˜¤ë¥˜ ë°œìƒ")
                    print(f"     ì˜¤ë¥˜: {result['error']}")
            print()  # ë¹ˆ ì¤„
        else:
            # ìˆœì°¨ ì‹¤í–‰ (ë‹¨ì¼ ê°œì²´ ë˜ëŠ” ë³‘ë ¬ ë¹„í™œì„±í™”)
            for member in members_to_train:
                steps_to_train = min(training_chunk, total_timesteps - member.training_steps)
                
                print(f"\nTraining Member {member.member_id:02d} for {steps_to_train} steps...")
                print(f"  Current steps: {member.training_steps}/{total_timesteps}")
                print(f"  Hyperparameters: {member.hyperparameters}")
                
                # ê°œì²´ í•™ìŠµ (training_chunkë§Œí¼) - ìˆœì°¨ ì‹¤í–‰
                member_dict = {
                    'member_id': member.member_id,
                    'best_distance': member.best_distance,
                    'current_distance': member.current_distance,
                    'training_steps': member.training_steps,
                    'log_dir': member.log_dir,
                    'model_path': member.model_path
                }
                result = train_single_member_chunk((
                    member_dict, base_config, log_dir_base, str(device), 
                    steps_to_train, member.hyperparameters
                ))
                
                # ìˆœì°¨ ì‹¤í–‰ì—ì„œëŠ” PBT ì—…ë°ì´íŠ¸
                if 'error' not in result and pbt is not None:
                    eval_data = result.get('eval_data')
                    member = pbt.population[result['member_id']]
                    
                    # ì¡°ê¸° ì¢…ë£Œ ì •ë³´ ì—…ë°ì´íŠ¸
                    if result.get('early_stopped', False):
                        member.early_stopped = True
                        member.early_stop_reason = result.get('early_stop_reason', 'Unknown')
                        print(f"âš ï¸  Member {result['member_id']:02d}: {member.early_stop_reason}")
                        print(f"   Will be restarted with new hyperparameters in next iteration")
                    
                    pbt.update_member(
                        result['member_id'],
                        result['best_distance'],
                        result['current_distance'],
                        result['training_steps'],
                        result['log_dir'],
                        result['model_path'],
                        eval_data=eval_data
                    )
                    
                    if not result.get('early_stopped', False):
                        print(f"âœ… Member {result['member_id']:02d}: Best Distance: {result['best_distance']:.4f}, Steps: {result['training_steps']}")
                else:
                    print(f"âŒ Member {result['member_id']:02d}: Error: {result['error']}")
            
            # ìƒíƒœ ì¶œë ¥
            monitor.print_status(population_size, total_timesteps)
        
        # ì£¼ê¸°ì  ìƒíƒœ ì¶œë ¥ (exploit/explore ì „)
        current_time = time.time()
        if current_time - last_monitor_time >= monitor_interval:
            monitor.print_status(population_size, total_timesteps)
            last_monitor_time = current_time
        
        # ì£¼ê¸°ì  ìƒíƒœ ì¶œë ¥ (exploit/explore ì „)
        current_time = time.time()
        if current_time - last_monitor_time >= monitor_interval:
            monitor.print_status(population_size, total_timesteps)
            last_monitor_time = current_time
        
        # Exploit and Explore ì²´í¬
        if pbt.should_exploit_explore():
            print(f"\n{'='*60}")
            print(f"Exploit and Explore (Generation {pbt.generation})")
            print(f"{'='*60}")
            
            updates = pbt.exploit_and_explore()
            
            if updates:
                print(f"Updated {len(updates)} members:")
                for member_id, new_hyperparams in updates.items():
                    member = pbt.population[member_id]
                    print(f"  Member {member_id:02d}:")
                    print(f"    Old hyperparams: {member.hyperparameters}")
                    print(f"    New hyperparams: {new_hyperparams}")
                    
                    # ëª¨ë¸ ë³µì‚¬ (exploit)
                    best_member = pbt.get_best_member()
                    if best_member.model_path and os.path.exists(best_member.model_path):
                        # ì„±ëŠ¥ì´ ì¢‹ì€ ê°œì²´ì˜ ëª¨ë¸ì„ ë‚˜ìœ ê°œì²´ì— ë³µì‚¬
                        target_model_path = os.path.join(
                            log_dir_base, member.log_dir, 'model_best.pt'
                        )
                        os.makedirs(os.path.dirname(target_model_path), exist_ok=True)
                        import shutil
                        shutil.copy(best_member.model_path, target_model_path)
                        member.model_path = target_model_path
                        print(f"    Model copied from best member ({best_member.member_id:02d})")
            
            # PBT ìƒíƒœ ë° í‰ê°€ ë°ì´í„° ì €ì¥
            pbt.save_state(os.path.join(log_dir_base, 'pbt_state.json'))
            pbt.save_evaluation_data(os.path.join(log_dir_base, 'pbt_evaluation_data.json'))
        
        # ìµœê³  ì„±ëŠ¥ ê°œì²´ ì¶œë ¥
        best_member = pbt.get_best_member()
        print(f"\n{'='*60}")
        print(f"Current Best Member: {best_member.member_id:02d}")
        print(f"  Best Distance: {best_member.best_distance:.4f}")
        print(f"  Training Steps: {best_member.training_steps}")
        print(f"  Hyperparameters: {best_member.hyperparameters}")
        print(f"{'='*60}")
    
    # ìµœì¢… ê²°ê³¼
    print(f"\n{'='*60}")
    print("PBT Training Completed!")
    print(f"{'='*60}")
    
    best_member = pbt.get_best_member()
    print(f"\nBest Member: {best_member.member_id:02d}")
    print(f"Best Distance: {best_member.best_distance:.4f}")
    print(f"Best Hyperparameters:")
    for key, value in best_member.hyperparameters.items():
        print(f"  {key}: {value}")
    print(f"Model Path: {best_member.model_path}")
    
    # ìµœê³  í•˜ì´í¼íŒŒë¼ë¯¸í„° ì €ì¥
    best_config = pbt.get_member_config(best_member.member_id)
    best_config_path = os.path.join(log_dir_base, 'best_hyperparameters.yaml')
    with open(best_config_path, 'w', encoding='utf-8') as f:
        yaml.dump(best_config, f, default_flow_style=False, allow_unicode=True)
    
    print(f"\nBest hyperparameters saved to: {best_config_path}")
    print(f"Results directory: {log_dir_base}")
    
    # ìµœì¢… í‰ê°€ ë°ì´í„° ì €ì¥
    pbt.save_evaluation_data(os.path.join(log_dir_base, 'pbt_evaluation_data.json'))
    pbt.save_state(os.path.join(log_dir_base, 'pbt_state.json'))
    
    print(f"\nEvaluation data saved to: {os.path.join(log_dir_base, 'pbt_evaluation_data.json')}")
    print(f"Visualize results with: python visualize_pbt_results.py --log-dir {log_dir_base}")
    
    return log_dir_base, best_member


if __name__ == '__main__':
    import argparse
    
    parser = argparse.ArgumentParser(description='Train TDM with Population-based Training (PBT)')
    parser.add_argument('--config', type=str, default='config.yaml',
                       help='Path to config file')
    
    args = parser.parse_args()
    
    # Windowsì—ì„œ multiprocessingì„ ìœ„í•œ ì„¤ì •
    if sys.platform == 'win32':
        try:
            mp.set_start_method('spawn', force=True)
        except RuntimeError:
            pass
    
    train_pbt(args.config)

