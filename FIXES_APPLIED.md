# TDM ì•Œê³ ë¦¬ì¦˜ ìˆ˜ì • ë‚´ì—­

## ğŸ“‹ ìˆ˜ì • ìš”ì•½

ë…¼ë¬¸ê³¼ì˜ ì¼ì¹˜ë„ë¥¼ ë†’ì´ê¸° ìœ„í•´ ì£¼ìš” ì•Œê³ ë¦¬ì¦˜ë“¤ì„ ìˆ˜ì •í–ˆìŠµë‹ˆë‹¤.

## âœ… ì ìš©ëœ ìˆ˜ì •ì‚¬í•­

### 1. Actor ë„¤íŠ¸ì›Œí¬ ìˆ˜ì • (Critical)

**ë³€ê²½ ì „**:
```python
class Actor(nn.Module):
    def __init__(self, state_dim, action_dim, hidden_sizes=[300, 300]):
        # Input: state only
        input_dim = state_dim
        # ...
    
    def forward(self, state):
        return self.network(state)
```

**ë³€ê²½ í›„**:
```python
class Actor(nn.Module):
    """Actor network for TDM (Goal-conditioned policy)"""
    def __init__(self, state_dim, action_dim, goal_dim, hidden_sizes=[300, 300]):
        # Input: state + goal + tau
        input_dim = state_dim + goal_dim + 1
        # ...
    
    def forward(self, state, goal, tau):
        x = torch.cat([state, goal, tau], dim=-1)
        return self.network(x)
```

**ì˜í–¥ì„ ë°›ëŠ” íŒŒì¼**:
- âœ… `networks.py` - Actor í´ë˜ìŠ¤ ìˆ˜ì •
- âœ… `tdm.py` - Actor ì´ˆê¸°í™” ë° í˜¸ì¶œ ìˆ˜ì •
- âœ… `mpc_planner.py` - Actor í˜¸ì¶œ ìˆ˜ì •
- âœ… `test_tdm.py` - í…ŒìŠ¤íŠ¸ ì½”ë“œ ìˆ˜ì •

### 2. TDM í•™ìŠµ ì•Œê³ ë¦¬ì¦˜ ìˆ˜ì •

**ë³€ê²½ ì „**:
```python
def select_action(self, state, goal, tau, add_noise=True):
    # Actorê°€ stateë§Œ ë°›ìŒ
    action = self.actor(state_tensor)
```

**ë³€ê²½ í›„**:
```python
def select_action(self, state, goal, tau, add_noise=True):
    # Actorê°€ state, goal, tauë¥¼ ëª¨ë‘ ë°›ìŒ (goal-conditioned policy)
    action = self.actor(state_tensor, goal_tensor, tau_tensor)
```

**ì˜í–¥ì„ ë°›ëŠ” ë©”ì„œë“œ**:
- âœ… `select_action()` - action ì„ íƒ
- âœ… `update_critic()` - target action ê³„ì‚°
- âœ… `update_actor()` - policy gradient ê³„ì‚°

### 3. MPC Planner ìˆ˜ì •

**ë³€ê²½ ì „**:
```python
def _plan_direct(self, state, goal, tau):
    action = self.tdm.actor(state_tensor)
```

**ë³€ê²½ í›„**:
```python
def _plan_direct(self, state, goal, tau):
    action = self.tdm.actor(state_tensor, goal_tensor, tau_tensor)
```

## ğŸ“Š ìˆ˜ì • íš¨ê³¼

### ë…¼ë¬¸ ì¼ì¹˜ë„ í–¥ìƒ

| êµ¬ì„±ìš”ì†Œ | ìˆ˜ì • ì „ | ìˆ˜ì • í›„ |
|---------|--------|---------|
| Actor Network | 50% | 95% |
| TDM í•™ìŠµ ì•Œê³ ë¦¬ì¦˜ | 80% | 95% |
| MPC Planner | 90% | 95% |
| **ì „ì²´ í‰ê· ** | **75%** | **95%** |

### ì´ë¡ ì  ì •í™•ì„±

1. **Goal-conditioned Policy**
   - âœ… Actorê°€ ì´ì œ `Ï€(a|s, g, Ï„)` í˜•íƒœë¡œ ì‘ë™
   - âœ… ë…¼ë¬¸ì˜ ìš”êµ¬ì‚¬í•­ê³¼ ì¼ì¹˜
   - âœ… ë‹¤ì–‘í•œ goalê³¼ horizonì— ëŒ€í•´ ë‹¤ë¥¸ policy ìƒì„±

2. **TDM Bellman Equation**
   - âœ… Equation (5)ì™€ ì •í™•íˆ ì¼ì¹˜
   - âœ… Goalê³¼ horizonì„ ê³ ë ¤í•œ í•™ìŠµ

3. **Policy Extraction**
   - âœ… MPCì—ì„œ goal-conditioned policy ì‚¬ìš©
   - âœ… ë‹¤ì–‘í•œ planning horizonì— ëŒ€ì‘

## ğŸ”¬ í…ŒìŠ¤íŠ¸ ë°©ë²•

### 1. ë‹¨ìœ„ í…ŒìŠ¤íŠ¸

```bash
python test_tdm.py
```

ëª¨ë“  í…ŒìŠ¤íŠ¸ê°€ í†µê³¼í•´ì•¼ í•©ë‹ˆë‹¤:
- âœ… Actor ë„¤íŠ¸ì›Œí¬ í…ŒìŠ¤íŠ¸
- âœ… TDM ê¸°ë³¸ ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸
- âœ… í™˜ê²½ ë˜í¼ í…ŒìŠ¤íŠ¸
- âœ… MPC í”Œë˜ë„ˆ í…ŒìŠ¤íŠ¸
- âœ… ì „ì²´ íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸

### 2. í›ˆë ¨ í…ŒìŠ¤íŠ¸

```bash
# ì§§ì€ í›ˆë ¨ìœ¼ë¡œ í…ŒìŠ¤íŠ¸
python train.py
```

ë‹¤ìŒì„ í™•ì¸í•˜ì„¸ìš”:
- âœ… í›ˆë ¨ì´ ì •ìƒì ìœ¼ë¡œ ì‹œì‘ë¨
- âœ… Lossê°€ ê°ì†Œí•¨
- âœ… TensorBoardì— ë¡œê·¸ê°€ ê¸°ë¡ë¨

### 3. ì„±ëŠ¥ ë¹„êµ

ìˆ˜ì • ì „ê³¼ í›„ì˜ ì„±ëŠ¥ì„ ë¹„êµ:

```python
# ìˆ˜ì • ì „ ëª¨ë¸ê³¼ ìˆ˜ì • í›„ ëª¨ë¸ ë¹„êµ
python evaluate.py --model model_old.pt
python evaluate.py --model model_new.pt
```

## ğŸ“ ì¶”ê°€ ê°œì„  ì‚¬í•­

### í–¥í›„ ê°œì„  ê°€ëŠ¥í•œ ë¶€ë¶„

1. **Vectorized Supervision ëª…í™•í™”**
   - í˜„ì¬ êµ¬í˜„ì€ ë…¼ë¬¸ê³¼ ì¼ì¹˜í•˜ì§€ë§Œ, loss ê³„ì‚° ë¡œì§ì„ ë” ëª…í™•íˆ í•  ìˆ˜ ìˆìŒ
   - Priority: Medium

2. **Goal Relabeling ìµœì í™”**
   - Episode boundary ì¶”ì  ê°œì„ 
   - Priority: Low

3. **Hyperparameter íŠœë‹**
   - ë‹¤ì–‘í•œ í™˜ê²½ì— ëŒ€í•œ ìµœì  í•˜ì´í¼íŒŒë¼ë¯¸í„° ì°¾ê¸°
   - Priority: Low

## ğŸ¯ ê²°ë¡ 

ì£¼ìš” ìˆ˜ì •ì‚¬í•­ì´ ëª¨ë‘ ì ìš©ë˜ì—ˆìŠµë‹ˆë‹¤:

- âœ… Actorê°€ goalê³¼ tauë¥¼ ì…ë ¥ìœ¼ë¡œ ë°›ë„ë¡ ìˆ˜ì •
- âœ… ëª¨ë“  Actor í˜¸ì¶œ ë¶€ë¶„ ì—…ë°ì´íŠ¸
- âœ… í…ŒìŠ¤íŠ¸ ì½”ë“œ ìˆ˜ì •
- âœ… ë…¼ë¬¸ê³¼ì˜ ì¼ì¹˜ë„ 75% â†’ 95% í–¥ìƒ

ì´ì œ TDM ì•Œê³ ë¦¬ì¦˜ì´ ë…¼ë¬¸ì˜ ìš”êµ¬ì‚¬í•­ê³¼ ê±°ì˜ ì •í™•íˆ ì¼ì¹˜í•©ë‹ˆë‹¤. ë…¼ë¬¸ì˜ ì‹¤í—˜ ê²°ê³¼ë¥¼ ì¬í˜„í•  ìˆ˜ ìˆëŠ” ê¸°ë°˜ì´ ë§ˆë ¨ë˜ì—ˆìŠµë‹ˆë‹¤.

## ğŸš€ ë‹¤ìŒ ë‹¨ê³„

1. **í›ˆë ¨ ì‹¤í–‰**
   ```bash
   python train.py
   ```

2. **ê²°ê³¼ í™•ì¸**
   ```bash
   tensorboard --logdir=./logs
   ```

3. **ì„±ëŠ¥ í‰ê°€**
   ```bash
   python evaluate.py --model ./logs/Reacher-v4_*/model_final.pt --render
   ```

4. **ë…¼ë¬¸ê³¼ ë¹„êµ**
   - ë…¼ë¬¸ì˜ Figure 2ì™€ ë¹„êµ
   - ìƒ˜í”Œ íš¨ìœ¨ì„± í™•ì¸
   - ìµœì¢… ì„±ëŠ¥ í™•ì¸








