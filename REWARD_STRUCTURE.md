# TDM ë³´ìƒ êµ¬ì¡° ë¶„ì„ ë° ê°œì„ ì•ˆ

## ğŸ“Š í˜„ì¬ ë³´ìƒ êµ¬ì¡°

### í˜„ì¬ êµ¬í˜„

```python
def compute_reward(self, state, goal):
    current_goal = self.goal_extractor(state)
    distance = np.abs(current_goal - goal).sum()
    return -distance  # ë‹¨ìˆœíˆ ìŒìˆ˜ ê±°ë¦¬
```

### ë¬¸ì œì 

1. **Mean Rewardì™€ Mean Distanceê°€ ì¤‘ë³µ**
   - Mean Reward = -Mean Distance
   - ë™ì¼í•œ ì •ë³´ë¥¼ ë‘ ë²ˆ ë¡œê¹…
   - ë¶ˆí•„ìš”í•œ ì¤‘ë³µ

2. **ë³´ìƒì˜ ì˜ë¯¸ê°€ ëª¨í˜¸í•¨**
   - ìŒìˆ˜ ë³´ìƒì€ ì§ê´€ì ì´ì§€ ì•ŠìŒ
   - ê±°ë¦¬ ê¸°ë°˜ ë³´ìƒë§Œìœ¼ë¡œëŠ” í•™ìŠµì´ ì–´ë ¤ìš¸ ìˆ˜ ìˆìŒ

## ğŸ’¡ ê°œì„  ë°©ì•ˆ

### ë°©ì•ˆ 1: ê±°ë¦¬ ê¸°ë°˜ ë³´ìƒ ìœ ì§€ (í˜„ì¬)

**ì¥ì **:
- ë…¼ë¬¸ê³¼ ì¼ì¹˜
- ë‹¨ìˆœí•˜ê³  ëª…í™•

**ë‹¨ì **:
- Mean Rewardì™€ Mean Distance ì¤‘ë³µ
- ìŒìˆ˜ ë³´ìƒìœ¼ë¡œ ì¸í•œ ì§ê´€ì„± ë¶€ì¡±

**ê°œì„ **:
- Mean Reward ë¡œê¹… ì œê±°
- Mean Distanceë§Œ ì‚¬ìš©

### ë°©ì•ˆ 2: ê±°ë¦¬ ê¸°ë°˜ ë³´ìƒ + ì„±ê³µ ë³´ë„ˆìŠ¤

```python
def compute_reward(self, state, goal):
    current_goal = self.goal_extractor(state)
    distance = np.abs(current_goal - goal).sum()
    
    # ê¸°ë³¸ ë³´ìƒ: ìŒìˆ˜ ê±°ë¦¬
    reward = -distance
    
    # ì„±ê³µ ë³´ë„ˆìŠ¤
    if distance < self.success_threshold:
        reward += self.success_bonus
    
    return reward
```

**ì¥ì **:
- ì„±ê³µ ì‹œ ê¸ì •ì  ë³´ìƒ
- ë” ëª…í™•í•œ í•™ìŠµ ì‹ í˜¸
- Mean Rewardì™€ Mean Distanceê°€ ë‹¤ë¥¸ ì˜ë¯¸

**ë‹¨ì **:
- ë…¼ë¬¸ê³¼ ë‹¤ë¥¼ ìˆ˜ ìˆìŒ
- ì¶”ê°€ í•˜ì´í¼íŒŒë¼ë¯¸í„° í•„ìš”

### ë°©ì•ˆ 3: ìŠ¤ì¼€ì¼ëœ ë³´ìƒ

```python
def compute_reward(self, state, goal):
    current_goal = self.goal_extractor(state)
    distance = np.abs(current_goal - goal).sum()
    
    # ë³´ìƒ ìŠ¤ì¼€ì¼ë§
    reward = -distance * self.reward_scale
    
    return reward
```

**ì¥ì **:
- ë³´ìƒ ë²”ìœ„ ì¡°ì • ê°€ëŠ¥
- í•™ìŠµ ì•ˆì •ì„± í–¥ìƒ

**ë‹¨ì **:
- ì—¬ì „íˆ ì¤‘ë³µ ë¬¸ì œ ì¡´ì¬

## ğŸ¯ ê¶Œì¥ ê°œì„ ì•ˆ

### ì˜µì…˜ 1: ë¡œê¹…ë§Œ ê°œì„  (ê°„ë‹¨)

Mean Reward ë¡œê¹…ì„ ì œê±°í•˜ê³  Mean Distanceë§Œ ì‚¬ìš©:

```python
# ì œê±°
writer.add_scalar('eval/mean_reward', ...)

# ìœ ì§€
writer.add_scalar('eval/mean_distance', ...)
```

### ì˜µì…˜ 2: ë³´ìƒ êµ¬ì¡° ê°œì„  (ê¶Œì¥)

ê±°ë¦¬ ê¸°ë°˜ ë³´ìƒì— ì„±ê³µ ë³´ë„ˆìŠ¤ ì¶”ê°€:

```python
def compute_reward(self, state, goal):
    current_goal = self.goal_extractor(state)
    distance = np.abs(current_goal - goal).sum()
    
    # ê¸°ë³¸ ë³´ìƒ: ìŒìˆ˜ ê±°ë¦¬
    reward = -distance * self.reward_scale
    
    # ì„±ê³µ ë³´ë„ˆìŠ¤ (ì„ íƒì‚¬í•­)
    if distance < 0.1:
        reward += 10.0
    
    return reward
```

**íš¨ê³¼**:
- Mean Reward: ì„±ê³µ ì‹œ ì–‘ìˆ˜, ì‹¤íŒ¨ ì‹œ ìŒìˆ˜
- Mean Distance: ëª©í‘œê¹Œì§€ì˜ ì‹¤ì œ ê±°ë¦¬
- ë‘ ë©”íŠ¸ë¦­ì´ ì„œë¡œ ë‹¤ë¥¸ ì •ë³´ ì œê³µ

## ğŸ“ ë…¼ë¬¸ê³¼ì˜ ë¹„êµ

### ë…¼ë¬¸ì˜ ë³´ìƒ êµ¬ì¡°

ë…¼ë¬¸ì—ì„œëŠ” TDMì˜ ë³´ìƒ í•¨ìˆ˜ë¥¼ ëª…í™•íˆ ì •ì˜í•˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ:

1. **TDM Q-function**:
   ```
   Q(s, a, g, Ï„) = -||f(s, a, g, Ï„) - g||
   ```
   - ìŒìˆ˜ ê±°ë¦¬ ì‚¬ìš©

2. **ì‹¤ì œ task reward**:
   - ë…¼ë¬¸ì—ì„œëŠ” task-specific reward ì‚¬ìš©
   - ì˜ˆ: ReacherëŠ” end-effectorì™€ ëª©í‘œì˜ ê±°ë¦¬

### í˜„ì¬ êµ¬í˜„ì˜ ë¬¸ì œ

í˜„ì¬ êµ¬í˜„ì€ TDMì˜ ë‚´ë¶€ ë³´ìƒê³¼ task rewardë¥¼ ë™ì¼í•˜ê²Œ ì‚¬ìš©í•˜ê³  ìˆìŠµë‹ˆë‹¤:

```python
# TDM ë‚´ë¶€: ê±°ë¦¬ ê¸°ë°˜
Q(s, a, g, Ï„) = -||f(s, a, g, Ï„) - g||

# Task reward: ë™ì¼í•˜ê²Œ ê±°ë¦¬ ê¸°ë°˜
reward = -distance(s, goal)
```

## ğŸ”§ êµ¬ì²´ì ì¸ ê°œì„  ì œì•ˆ

### 1. ë¡œê¹… ê°œì„  (ì¦‰ì‹œ ì ìš© ê°€ëŠ¥)

```python
# train.py, evaluate.pyì—ì„œ
# Mean Reward ë¡œê¹… ì œê±°
# Mean Distanceë§Œ ì‚¬ìš©
```

### 2. ë³´ìƒ êµ¬ì¡° ê°œì„  (ì„ íƒì‚¬í•­)

```python
# env_wrapper.py
def compute_reward(self, state, goal):
    current_goal = self.goal_extractor(state)
    distance = np.abs(current_goal - goal).sum()
    
    # ê¸°ë³¸ ë³´ìƒ: ìŒìˆ˜ ê±°ë¦¬ (ìŠ¤ì¼€ì¼ë§)
    reward = -distance * self.reward_scale
    
    # ì„±ê³µ ë³´ë„ˆìŠ¤ (ì„ íƒì‚¬í•­)
    if distance < 0.1:
        reward += 10.0
    
    return reward
```

## ğŸ“Š ë©”íŠ¸ë¦­ ë¹„êµ

### í˜„ì¬ (ì¤‘ë³µ)

| ë©”íŠ¸ë¦­ | ê°’ | ì˜ë¯¸ |
|--------|-----|------|
| Mean Reward | -5.0 | í‰ê·  ë³´ìƒ |
| Mean Distance | 5.0 | í‰ê·  ê±°ë¦¬ |
| **ê´€ê³„** | **-1ë°°** | **ë™ì¼í•œ ì •ë³´** |

### ê°œì„  í›„ (ì¤‘ë³µ ì œê±°)

| ë©”íŠ¸ë¦­ | ê°’ | ì˜ë¯¸ |
|--------|-----|------|
| ~~Mean Reward~~ | ~~-5.0~~ | ~~ì œê±°ë¨~~ |
| Mean Distance | 5.0 | í‰ê·  ê±°ë¦¬ |
| Success Rate | 0.8 | ì„±ê³µë¥  |

### ê°œì„  í›„ (ë³´ìƒ êµ¬ì¡° ë³€ê²½)

| ë©”íŠ¸ë¦­ | ê°’ | ì˜ë¯¸ |
|--------|-----|------|
| Mean Reward | 2.0 | í‰ê·  ë³´ìƒ (ì„±ê³µ ì‹œ ì–‘ìˆ˜) |
| Mean Distance | 5.0 | í‰ê·  ê±°ë¦¬ |
| Success Rate | 0.8 | ì„±ê³µë¥  |

## ğŸ¯ ê²°ë¡ 

ì‚¬ìš©ìì˜ ì§€ì ì´ ì •í™•í•©ë‹ˆë‹¤:

1. **í˜„ì¬ ë¬¸ì œ**: Mean Reward = -Mean Distance (ì¤‘ë³µ)
2. **í•´ê²° ë°©ì•ˆ**:
   - ì˜µì…˜ 1: Mean Reward ë¡œê¹… ì œê±° (ê°„ë‹¨)
   - ì˜µì…˜ 2: ë³´ìƒ êµ¬ì¡° ê°œì„  (ë” ë‚˜ì€ í•™ìŠµ)

ì–´ë–¤ ë°©í–¥ìœ¼ë¡œ ê°œì„ í• ê¹Œìš”?







