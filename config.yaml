# Temporal Difference Models (TDM) Configuration
# 
# 이 파일은 Reacher-v5에 대한 기본 설정입니다.
# 다른 환경을 사용하려면 다음 설정 파일을 사용하세요:
#   - config_reacher.yaml (Reacher-v5)
#   - config_pusher.yaml (Pusher-v5)
#   - config_halfcheetah.yaml (HalfCheetah-v5)
#   - config_ant.yaml (Ant-v5)
#
# 또는 train_with_curriculum.py 실행 시 --config 옵션으로 지정:
#   python train_with_curriculum.py --config config_pusher.yaml

# Environment settings
env:
  name: "Reacher-v5"  # Available: Reacher-v5, Pusher-v5, HalfCheetah-v5, Ant-v5
  max_episode_steps: 50
  
# Task-specific settings
task:
  # For reaching tasks (Reacher, Pusher)
  goal_space: "end_effector"  # or "full_state"
  
  # For locomotion tasks (Cheetah, Ant)
  # Cheetah: goal is velocity
  # Ant: goal is position or position+velocity
  locomotion_task_type: "position"  # "position" or "position_velocity"
  
  # Goal sampling
  goal_sampling_strategy: "future"  # "future", "buffer", or "uniform"
  
  # For ant position+velocity task
  position_weight: 0.1
  velocity_weight: 0.9

# TDM hyperparameters
tdm:
  tau_max: 25  # Maximum planning horizon
  distance_metric: "L1"  # "L1" or "L2"
  vectorized_supervision: true  # Use vectorized supervision for better performance
  reward_scale: 1.0
  
# Network architecture
network:
  actor:
    hidden_sizes: [300, 300]
    activation: "relu"
    output_activation: "tanh"
    
  critic:
    hidden_sizes: [300, 300]
    activation: "relu"
    
# Training hyperparameters
training:
  total_timesteps: 1000000
  learning_rate_actor: 0.0001
  learning_rate_critic: 0.001
  batch_size: 128
  buffer_size: 1000000
  updates_per_step: 10  # Number of gradient updates per environment step
  polyak: 0.999  # Soft target update coefficient
  
  # Learning rate decay (prevents performance collapse)
  use_lr_decay: false  # Enable learning rate decay
  lr_decay_rate: 0.9995  # Decay rate per decay_steps
  lr_decay_steps: 10000  # Decay every N steps
  min_lr_actor: 0.00001  # Minimum learning rate for actor
  min_lr_critic: 0.0001  # Minimum learning rate for critic
  
  # Gradient clipping (prevents gradient explosion)
  grad_clip: 1.0  # Clip gradients to this value (None to disable)
  
  # Exploration
  noise_type: "normal"
  noise_std: 0.2
  noise_decay: 0.9995
  noise_decay_steps: 100000
  min_noise_std: 0.01  # Minimum exploration noise (prevents over-exploitation)
  
  # Evaluation and early stopping
  eval_frequency: 5000  # Evaluate every N steps
  eval_episodes: 10
  patience: null  # Early stopping patience (null to disable, e.g., 10 means stop after 10 evals without improvement)
  
  # Policy Collapse Detection
  detect_policy_collapse: true  # Enable policy collapse detection
  collapse_detection:
    window_size: 5  # Number of recent evaluations to track
    collapse_threshold: 0.3  # Performance degradation threshold (0.3 = 30% worse)
    min_evaluations: 3  # Minimum evaluations before detecting collapse
    stability_threshold: 0.5  # Coefficient of variation threshold for instability
  
  # Curriculum Learning
  use_curriculum: true  # Enable curriculum learning
  curriculum:
    initial_difficulty: 0.1  # 초기 난이도 (0.0 ~ 1.0)
    final_difficulty: 1.0  # 최종 난이도 (0.0 ~ 1.0)
    type: "distance"  # "distance" or "complexity"
    schedule: "linear"  # "linear", "exponential", "step"
  
  # Warm-up Period
  use_warmup: true  # Enable warm-up period
  warmup:
    steps: 10000  # Warm-up 기간 (스텝 수)
    initial_noise_std: 0.5  # 초기 노이즈 표준편차
    final_noise_std: 0.2  # 최종 노이즈 표준편차
    initial_lr_multiplier: 0.1  # 초기 학습률 배수
    final_lr_multiplier: 1.0  # 최종 학습률 배수
  
# Policy extraction (MPC)
mpc:
  method: "direct"  # "direct" or "optimization"
  horizon: 15  # Planning horizon for MPC
  num_samples: 10000  # For stochastic optimization
  
# Logging
logging:
  log_dir: "./logs"
  tensorboard: true
  save_frequency: 10000  # Save model every N steps
  log_frequency: 100  # Log metrics every N steps

# Seed for reproducibility
seed: 42

# Population-based Training (PBT)
pbt:
  enabled: true  # PBT를 기본 학습 방법으로 사용
  population_size: auto  # 동시에 실행할 개체 수 (auto: 자동 결정, 숫자: 수동 설정)
  exploit_frequency: 10000  # 몇 스텝마다 exploit/explore 수행
  exploit_threshold: 0.25  # 하위 25%를 교체
  explore_perturbation: 0.2  # 하이퍼파라미터 변형 정도 (20%)
  max_generations: 100  # 최대 세대 수
  training_chunk: 5000  # 한 번에 학습할 스텝 수 (exploit/explore 주기보다 작아야 함)
  use_parallel: true  # 병렬 실행 활성화 (빠른 결과 확인)
  num_workers: null  # 병렬 워커 수 (null이면 자동 설정: min(population_size, cpu_count-1))
  monitor_interval: 5  # 진행 상황 출력 간격 (초)
  hyperparameter_ranges:  # 조정 가능한 하이퍼파라미터 범위
    learning_rate_actor: [0.00005, 0.0003]
    learning_rate_critic: [0.0005, 0.003]
    tau_max: [15, 30]
    batch_size: [64, 256]
    updates_per_step: [5, 20]
    noise_std: [0.1, 0.3]
    reward_scale: [0.1, 10.0]

